
== Introducción


Hay muchos ejemplos de código, en algunos casos el lenguaje está especificado si no es así se trata de pseudocódigo. Podía usar un estándar, por ejemplo un pseudocódigo estilo Python, C, o Go. En la mayoría de los casos usaré un pseudocódigo similar a C (con sus llaves y punto y comas), pero cuando son ejemplos sencillos uso una sintaxis similar a Python para evitar el "ruido" impuesto por los +{...}+ y +;+.

El código está en <<source_code>>


La terminología es fundamental para entender la documentación técnica, me tomé el trabajo de indicar el nombre inglés que se suele usar para las diferentes.


=== Concurrencia

Programas concurrentes son los que se ejecutan como procesos o hilos asincrónicos y que acceden a recursos compartidos. Los programas concurrentes pueden o no ejecutarse en paralelo, esto último es sólo una forma de ejecutarlos, no necesariamente un programa concurrente ha de ejecutarse en paralelo, puede hacerlo en un único procesador. Los problemas y soluciones a los accesos concurrentes son similares para ambos tipos de ejecuciones.

Pero con la popularización de las arquitecturas de múltiples procesadores y núcleos ha crecido enormemente el interés por el tema de concurrencia, esto está afectando también a los lenguajes más populares y seguramente les obligará a cambios radicales en los siguientes años para adaptarlos a los exigencias modernas.

//

Hace unas décadas el interés en el desarrollo de mecanismos a exclusión mutua estaba limitado a aquellos desarrolladores de sistemas operativos que por su propia características experimentaron problemas de sincronización desde sus inicios. Pero el interés creció en los últimos años con la popularización de arquitecturas de multiprocesadores y el lógico interés en que los programas los aprovechen al máximo. Así se desarrollaron y afinaron protocolos, algoritmos, técnicas y lenguajes para facilitar y evitar errores de programación, es lo que conocemos genéricamente como _programación concurrente_.

Los programas concurrentes pueden o no ejecutarse en paralelo, esto último es sólo una forma de ejecutar programas concurrentes. No necesariamente un programa concurrente ha de ejecutarse en paralelo. Por otro lado, los programas concurrentes que se ejecutan en un único procesador son  víctimas de los mismos problemas. Los programas concurrentes, y por extensión los paralelos, constan de varios hilos de ejecución independientes o _asincrónicos_. Sean en paralelo o en un único procesador, la ejecución de las instrucciones de cada uno de estos hilos se intercalan (_interleaving_), dicha intercalación de instrucciones produce los mismos problemas en ambos casos.


=== Procesos

Los programas en ejecución se denominan procesos, son un elemento de gestión muy importante del sistema operativo. Los procesos tienen dos partes bien diferenciadas, la imagen de memoria y las tablas de control de procesos.

Se denomina imagen de memoria al código y datos del programa en la memoria RAM. Se diferencian tres partes:

- El código o texto: El programa ejecutable cargado en memoria.
- Datos estáticos: La zona de memoria donde se almacenan las variables estáticas del programa (normalmente las variables globales).
- La pila (_stack_): La zona donde se almacenan los argumentos a funciones, su valor de retorno y las variables automáticas (locales).
- Zona de memoria dinámica (_heap + malloc_): La zona de almacenamiento de memoria asignada dinámicamente a los proceso (llamada también _anónima_ en Linux, habitualmente por llamadas a +malloc+).

Para poder gestionar a los procesos los sistemas operativos mantienen complejas estructuras de datos sobre la información de contexto y estado de cada proceso. Por ejemplo los valores de los registros del procesador cuando el proceso fue interrumpido, las tablas de memoria (tablas de páginas), estado de entradas y salidas, información de propiedad, etc.


==== Estados de los procesos

Durante su ciclo de vida los estados pasan por tres estados básicos:

- Ejecución: El proceso se está ejecutando en una CPU. Como máximo hay un proceso en ejecución por cada procesador.

- Listos para ejecutar (o simplemente _listos_): El proceso no se está ejecutando pero puede pasar a ejecución inmediatamente. El sistema mantiene colas de los procesos en este estado ordenadas por diferentes criterios (prioridad, tiempo de ejecución, afinidad a un procesador, etc.).

- Bloqueados (también llamados _suspendidos_ en Linuxfootnote:[En la bibliografía académica _suspendido_ es otro estado diferente, cuando un proceso ha sido expulsado de la memoria RAM.]):  Los procesos en este estado no están listos para ser ejecutados, tienen que esperar que ocurra un evento para estar nuevamente listos. El sistema mantiene diferentes colas clasificadas por el tipo de evento que esperan los procesos en ella.


.Estados de procesos
image::processes_states.png[width="440", align="center"]

Los procesos pasan a bloqueados porque solicitaron una operación al sistema operativo (vía _llamadas de sistema_) y deben esperar a que la operación acabe o esperar un evento de algún dispositivo. Cuando la operación acaba el proceso es movido a la cola de listos para ejecutar.

==== Scheduler
La transición entre _listos_ y  en _ejecución_ lo decide el módulo _scheduler_ del sistema operativo. Se ejecuta cada vez que un proceso se bloquea, cuando se produjo algún evento (habitualmente interrupciones de hardware generadas por dispositivos) o un proceso pasó de bloqueado a listo. Para evitar que un proceso pueda tomar y no soltar el control del procesador se añaden interrupciones programadas en un reloj.

Cada pocos milisegundosfootnote:[Varía entre 100 a 1000 veces por segundo, en Linux por defecto es 250 Hz.] el reloj genera una interrupción que hace que el procesador pase el control al _scheduler_ para que tome la decisión de qué proceso debe ejecutarse. Dado que el sistema es capaz de quitar de ejecución al proceso aunque éste no genere ninguna llamda de sistema se dice que el sistema es _apropiativo_ (_preemptive_).

En cada uno de estos casos el _scheduler_ tiene que decidir cuál de los siguientes procesos se ejecutará, debe hacerlo entre unas pocas cientos de veces a decenas de miles de veces por segundo. El requisito clave del _scheduler_ es que la decisión la haga lo más rápido posible, por ello se usan sofisticados algoritmos de colas para que la selección sea lo más rápido posible y con complejidad _O(1)_ (i.e. se selecciona el primer elemento de la cola).

Además el _scheduler_ debe asegurar distribuir los tiempos de procesador equitativamente, tener en cuenta las diferentes prioridades y evitar que se produzcan _esperas infinitas_ de procesos que nunca son seleccionados para ejecución. Se usa como base un algoritmo basado en turnos rotatorios (_round robin_), a cada proceso se le asigna un tiempo máximo de ejecución (el _cuanto_ o _quantum_), pasado ese tiempo el proceso es interrumpido y si hay otro proceso listo se hace un _cambio de contexto_ (_context switching_).

==== Cambios de contexto
Los _cambios de contexto_ son costosos, el sistema operativo y el procesador deben trabajar juntos para:

1. Guardar los registros del procesador para restaurarlos cuando el proceso vuelva a ejecución.

2. Marcar como inválidas las entradas de caché de las tablas de página (los _TLB_, _Translation Lookaside Buffer_) y copiar las entradas modificadas a sus correspondientes entradas de las tablas de página del proceso (_TLB flushing_).

3. Restaurar los registros del procesador y tablas de páginas.

Hay costes adicionales, como los _fallos de caché_ -de memoria RAM y los _TLB_- porque el procesador accede a direcciones físicas y tablas de páginas diferentes. El coste es todavía algo superior si el proceso se ejecutó antes en un procesador diferente. El _scheduler_ está diseñado y en constante evolución para tomar en cuenta todos estos parámetros conseguir objetivos contradictorios: aumentar el rendimiento global del sistema, asegurar buenos tiempos de respuesta y evitar esperas prolongadas.

Los procesos no tienen el control del procesador ni cuándo se ejecutarán, dada la complejidad de las interacciones y eventos también es muy difícil predecir o repetir exactamente una secuencia de ejecución, por eso los _schedulers_ de sistemas operativos de uso general se dicen que son _no determinísticos_.

=== Hilos

Los procesos _tradicionales_ no comparten memoria por requisitos de seguridad. Es imposible que un proceso acceda a la memoria de otros salvo que se usen mecanismos ad-hoc para poder compartir segmentos (como el +shmget+ del estándar System V). A principios de la década de 1980 se empezaron a desarrollar programas, sobre todo interactivos, más complejos y que requerían responder a una multitud de eventos diferentesfootnote:[Por ejemplo un procesador de texto, hay que responder al teclado, otro módulo que se encarga de la paginación, otro del correcto ortográfico, etc.].

Este tipo de programación se denomina _dirigida por eventos_ (_event driven_) donde se _seleccionan_ los diferentes eventos dentro de un bucle y se llaman a las funciones correspondientes. La programación de este tipo es compleja para estructurar y asegurar que se ejecuta sin errores. De esta necesidad surgieron dos conceptos muy vigentes hoy y que en general se encuadran en lo que conocemos como _programación concurrente_.

Por un lado se desarrollaron librerías -sobre todo gráficas e interfaces de usuario- y lenguajes que facilitan la programación de diferentes módulos que se ejecutan independientemente de los demás. Hoy la conocemos como _programación asincrónica_.

Como una forma de facilitar aún más el desarrollo de módulos asincrónicos se desarrolló el concepto de hilos (_threads_) o _procesos ligeros_ (_light weight processes_). En vez de crear una copia de toda la imagen de memoria de un proceso cuando se crea uno nuevofootnote:[Como hace el +fork+ en Unix.] se mantiene la misma copia para ambos procesos salvo la pila (cada uno tiene su propio contexto de ejecución). Los hilos comparten el código, variables estáticas y la memoria asignada dinámicamente entre todos los creados por el mismo proceso _padre_.

Desde el punto de vista del _scheduler_ los hilos son idénticos a procesos independientes, cada uno de ellos -al igual que los procesos tradicionales- son _unidades de planificación_. Si los hilos se ejecutan en un sistema multiprocesador además de ejecutarse asincrónicamente pueden hacerlo en paralelo en diferentes procesadores. Por la popularización de los chips _multicore_ la programación con hilos se convirtió en una parte importante de la programación concurrrentefootnote:[Aunque muchos confunden la capacidad de ejecución asincrónica con paralelismo, de nuevo, el paralelismo es solo una forma de ejecución de programas concurrentes.].

Además de las facilidades que brinda a los programadores los hilos son más _baratos_ que los procesos. Consumen menos memoria y al no tener que copiar toda la memoria el tiempo de creación de nuevos hilos es mucho menor que el de procesos tradicionales. Tiene otras ventajas más sutiles, al compatir gran parte de la memoria entre os diferentes hilos el coste de los cambios de contexto es también menor, se invalidan y reemplazan menos entradas del _TLB_ y las líneas de caché.

==== Hilos ligeros
Antes que los sistemas operativos y sus librerías diesen soporte estándarfootnote:[Como lo hacen ahora POSIX Threads en entornos Unix.] para la programación asincrónica algunos lenguajes y sus máquinas virtuales implementaron sus propios _schedulers_ a nivel de aplicación para simular hilos. Los casos más conocidos son los hilos ligeros en la máquina vitual de Erlang y la antigua emulación de hilos en la máquina virtual de Java, los _green threads_.

Muchos lenguajes usan hilos ligeros para reducir el coste de la creación de hilos nativos del sistema operativo. En Go se denominan _goroutines_ y permiten crear hilos con muy pocas instrucciones (aseguran que tres) y consumo de memoria de muy pocos kilobytes. En otros lenguajes se suelen llamar _tasklets_, también suelen incluir esta capacidad los módulos de _programación asíncrona_ de lenguajes dinámicos.

Es importante recalcar que los hilos ligeros no son planificados por el _scheduler_ del sistema operativo sino internamente por el programa o máquina virtual. Esto implica que no pueden ejecutarse en paralelo, a menos que creen hilos nativos con este propósito (como hace Go o Erlang desde la versión _SMP_ R11B).







==== Intercalación
La ejecución




.Intercalado
image::interleaving.png[height="120", align="center"]


.Multiprocesamiento
image::multiprocessing.png[height="120", align="center"]









Los programadores estamos acostumbrados al modelo de _consistencia secuencial_ de los lenguajes de programación: una instrucción que está después de otra se ejecuta ejecuta a continuación de ésta. Una de las propiedades que distingue a la programación concurrente es que esta consistencia secuencial ya no se cumplefootnote:[Más adelante, en <<barriers>> veremos que las arquitecturas modernas de hardware tampoco aseguran por defecto la consistencia secuencial.].



=== Intercalado de instrucciones

La mayoría de los lenguajes de programación están diseñados para especificar y ejecutar las instrucciones secuencialmente. Tomemos la siguiente secuencia de instrucciones que se ejecutan en un programa con las variable +a+ y +b+ inicializadas a +0+

[source, python]
----
a = a + 1
b = b + a
print "a, b:", a, b
----

Por el modelo de consistencia secuencial es fácil deducir que el resultado de imprimir las tres variables será +1 1+. Si las dos asignaciones se repiten el resultado será +a, b: 2 3+, el siguiente +a, b: 3 6+, etc.

Ahora supongamos que este fragmento de código se ejecuta en procesos o hilos diferentes (+P+ y +Q+) sobre un sistema con un único procesador y que tanto +a+ como +b+ con _variables compartidas_. Se puede producir la siguiente intercalación de las instrucciones del programa:


----
Proceso P               Proceso Q

...
a = a + 1
                        a = a + 1
                        b = b + a
                        print "a, b:", a, b
                        ...
b = b + a
print "a, b:", a, b
----



El resultado de la ejecución de estas instrucciones será:

----
a, b: 2 2
a, b: 2 4
----

Ninguno de los valores es correcto. Si se ejecuta nuevamente el resultado podría ser diferente, depende del instante y orden en que cada proceso ejecuta las instrucciones en _secciones críticas_ del código que acceden a recursos u _objetos compartidos_ (en este caso variables). Este problema se denomina genéricamente como _condición de carrera_ (_race condition_). Es muy difícil detectar los _bugs_ causados por condiciones de carrera, habitualmente no son frecuentes porque la probabilidad de que ocurra es muy bajafootnote:[Al contrario de los ejemplos en este libro, diseñados de tal manera que se aumenta artificialmente la probabilidad de que ocurran estas condiciones de carrera.] y es muy difícil repetir el error con las mismas condicionesfootnote:[Recuerda que la planificación de CPU es no determinística en los sistemas operativos modernos.].

Esas dos líneas (o tres si contamos con el +print+ de ambos resultados) acceden a variables compartidas y que además tienen dependencias entre ellas: el resultado de +b+ depende de +a+. Las secuencias anteriores de _instrucciones_ no son _atómicas_, el proceso puede ser interrumpido y ejecutarse otro que modifica las mismas variables. Lo mismo puede ocurrir con instrucciones más básicas y sobre las que solemos hacer suposiciones erróneas:

    counter += 1

Se suele suponer que una operación tan básica como sumar una constante (o _literal_) a una variable no es interrumpible, pero no es así. El código ejecutable está compuesto por al menos tres instrucciones de procesador:

----
movl  counter(%rip), %eax
addl  $1, %eax
movl  %eax, counter(%rip)
----

Si se ejecuta dos veces el valor de +counter+ será +2+, es factible que se presente la siguiente condición de carrera ente dos procesos:

----
movl counter(%rip), %eax <1>
                        movl counter(%rip), %eax
                        addl $1, %eax
                        movl %eax, counter(%rip)
addl $1, %eax <2>
movl %eax, counter(%rip)
----

<1> Se almacena 0 en el registro eax.
<2> Aunque la variable ya tiene almacenado el valor +1+, el registro %eax sigue siendo 0.

En este caso el valor será +1+, se ha _perdido_ una operación. Es el problema más habitual. También pasa con lenguajes dinámicos y con compilación de _bytecode_ como Java o Python. El siguiente código es el generado por la compilación de Python, son cuatro instrucciones:

----
LOAD_GLOBAL   0 (counter)
LOAD_CONST    1 (1)
INPLACE_ADD
STORE_GLOBAL  0 (counter)
----

==== Ejemplos en diferentes lenguajes

Los siguientes programas  <<counter_c, en C>>, <<gocounter_go, Go>>, <<counter_java, Java>> y <<counter_py, Python>> hacen lo mismo: crean dos hilos que incrementan un contador compartido (+counter+) cuyo total debería ser diez millones. El resultado de sus ejecuciones son los siguientes:

[[counter_times]]
.Resultados y tiempos de CPU
----
$ time ./counter
Counter value: 5785131 Expected: 10000000
real    0m0.010s <1>
user    0m0.017s
sys     0m0.000s

$ time ./gocounter
Counter value: 5052927 Expected: 10000000
real    0m0.021s <1>
user    0m0.032s
sys     0m0.008s

$ time java Counter
Counter value: 4406963 Expected: 10000000
real    0m0.333s <1>
user    0m0.564s
sys     0m0.020s

$ time ./counter.py
Counter value: 7737979 Expected: 10000000
real    0m5.400s <2>
user    0m5.365s
sys     0m0.044s
----
<1> El tiempo de _reloj_ es menor al tiempo acumulado de CPU.
<2> El tiempo de _reloj_ es mayor al tiempo acumulado de CPU.


[NOTE]
.Sobre los tiempos de CPU
====
Compara los _tiempos de CPU_ con los _tiempos de reloj_. Salvo Python todos lo superan, se ejecutan en paralelo en dos CPUs por lo que por cada segundo de reloj corresponde a dos segundos de procesador. Los programas en Python no pueden ejecutarse simultáneamente en más de un procesador debido a al _Python Global Interpreter Lock_ (<<Sampson>>).
====

En los ejemplos anteriores se observa que en todos _perdieron_ hasta más de la mitad de los operaciones. El error se debe a la intercalación de instrucciones, éstas pueden ocurrir tanto en sistemas con un único procesador como con paralelismo. Una solución correcta de exclusión mutua es equivalente y funciona para ambos modos: el paralelismo es sólo un caso particular de la intercalación.





////

http://talks.golang.org/2012/waza.slide#6
Concurrency
Programming as the composition of independently executing processes.
(Processes in the general sense, not Linux processes. Famously hard to define.)

Parallelism
Programming as the simultaneous execution of (possibly related) computations.

Concurrency vs. parallelism
Concurrency is about dealing with lots of things at once.
Parallelism is about doing lots of things at once.
Not the same, but related.
Concurrency is about structure, parallelism is about execution.
Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.

Conclusion
Concurrency is powerful.
Concurrency is not parallelism.
Concurrency enables parallelism.
Concurrency makes parallelism (and scaling and everything else) easy.


https://existentialtype.wordpress.com/2011/03/17/parallelism-is-not-concurrency/
The first thing to understand is parallelism has nothing to do with concurrency.  Concurrency is concerned with nondeterministic composition of programs (or their components).  Parallelism is concerned with asymptotic efficiency of programs with deterministic behavior

////



=== Procesos

Intro a procesos.

=== Compartir memoria

Cómo comparten memoria los procesos.

=== Mecanismos de sincronización

Algoritmos.

Instrucciones hardware.


=== Hilos

Explicar hilos. Comparten memoria

=== Mecanismos de sincronización

fork
exec
clone
futex
mmap
posix_threads

POSIX Threads, bla bla.
