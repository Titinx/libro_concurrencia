== Procesos y concurrencia

Los programas en ejecución se denominan procesos, son los elementos de gestión centrales del sistema operativo. Desde el punto de vista del núcleofootnote:[El sistema operativo está formado por un núcleo, como Linux, y las librerías y herramientas necesarias para poder arrancar y ejecutar los procesos necesarios para el funcionamiento normal del sistema. El núcleo es el programa que se carga al arranque, inicializa el sistema y gestiona recursos y procesos ejecutándose con privilegios especiales del procesador.] del sistema operativo los procesos tienen dos partes bien diferenciadas, la imagen de memoria y las tablas de control de procesos.

Se denomina imagen de memoria al código y datos del programa en la memoria RAM. Se diferencian cuatro partes según el contenido de cada una de ellas:

- Código o texto: El programa ejecutable cargado en memoria.
- Datos: La zona de memoria donde se almacenan las constantes y variables estáticas del programa (normalmente las variables globales).
- Pila (_stack_): La zona donde se almacenan los argumentos de las funciones, sus valores de retorno y las variables automáticas (locales).
- Zona de memoria dinámica (_heap + malloc_): La zona de almacenamiento de memoria asignada dinámicamente a los proceso (llamada también _anónima_ en Linux, habitualmente por llamadas a +malloc+).

Para gestionar los procesos los sistemas mantienen complejas estructuras de datos sobre la información de contexto y estado de cada uno. Por ejemplo los valores de los registros del procesador cuando el proceso fue interrumpido, las tablas de páginas de memoria, estado de entradas y salidas, información de propiedad, etc.

****
La forma estándar de crear proceso en Unix es la llamada de sistema +fork+.

    pid = fork();

Esta llamada de sistema crea un proceso idéntico al proceso que la ejecutó, el nuevo proceso es _hijo_ del original y ambos continúan su ejecución independientemente. La imagen de memoria y las tablas de entrada/salida del hijo son copiasfootnote:[Se usa la técnica _copy-on-write_ (_COW_) para evitar copiar toda la memoria, se copia bajo demanda solos aquellas modificadas por alguno de los procesos. Se consigue más eficiencia y ahorro de memoria RAM.] de las de padre, la única forma de diferenciarlos en por el valor de retorno de la llamada, en el ejemplo almacenado en la variable +pid+. Si +pid+ vale cero se trata del proceso hijo, si es mayor que cero es el proceso _padre_.
****


=== Estados de procesos

Durante su ciclo de vida los estados pasan por tres estados básicos:

- Ejecución: El proceso se está ejecutando en una CPU. Como máximo hay un proceso en ejecución por cada procesador.

- Listos para ejecutar (o simplemente _listos_): El proceso no se está ejecutando pero puede pasar a ejecución inmediatamente. El sistema mantiene colas de los procesos en este estado ordenadas por diferentes criterios (prioridad, tiempo de ejecución, afinidad a un procesador, etc.).

- Bloqueados (también llamados _suspendidos_ en Linuxfootnote:[En la bibliografía académica _suspendido_ es otro estado diferente, cuando un proceso ha sido expulsado de la memoria RAM.]):  Los procesos en este estado no están listos para ser ejecutados, tienen que esperar que ocurra un evento para estar nuevamente listos. El sistema mantiene diferentes colas clasificadas por el tipo de evento que esperan los procesos en este estado.


.Estados de procesos
image::processes_states.png[width="440", align="center"]

Los procesos pasan a bloqueados porque solicitaron una operación al sistema operativo (vía _llamadas de sistema_) y deben esperar a que la operación acabe o esperar un evento de algún dispositivo. Cuando la operación acaba el proceso es movido a la cola de listos para ejecutar.

=== _Scheduler_
La transición entre _listos_ y  en _ejecución_ lo decide el módulo _scheduler_ o _planificador a corto plazo_ del núcleo. Se ejecuta cada vez que un proceso se bloquea, cuando se produjo algún evento (habitualmente interrupciones de hardware generadas por dispositivos) o un proceso pasó de bloqueado a listo. Para evitar que un proceso pueda tomar y no soltar el control del procesador se añaden interrupciones programadas en un reloj.

Cada pocos milisegundosfootnote:[Varía entre 100 a 1000 veces por segundo, en Linux por defecto es 250 Hz.] el reloj genera una interrupción que hace el _scheduler_ tome el control y decida qué proceso debe ejecutarse a continuación. Dado que el sistema es capaz de quitar de ejecución al proceso aunque éste no genere ninguna llamada de sistema se dice que el planificador es _apropiativo_ (_preemptive_).

En cada uno de estos casos el _scheduler_ tiene que decidir cuál de los siguientes procesos se ejecutará, debe hacerlo entre unas pocas cientos de veces a decenas de miles de veces por segundo. El requisito clave del _scheduler_ es que la decisión la haga lo más rápido posible, por ello se usan sofisticados algoritmos de colas para que la selección sea lo más rápido posible y con complejidad _O(1)_ (i.e. se selecciona el primer elemento de la cola).

Además el _scheduler_ debe asegurar distribuir los tiempos de procesador equitativamente, tener en cuenta las diferentes prioridades y evitar que se produzcan _esperas infinitas_ de procesos que nunca son seleccionados para ejecución. Se usa como base un algoritmo basado en turnos rotatorios (_round robin_), a cada proceso se le asigna un tiempo máximo de ejecución (el _cuanto_ o _quantum_), pasado ese tiempo el proceso es interrumpido y si hay otro proceso listo se hace un _cambio de contexto_ (_context switching_).

==== Cambios de contexto
Los _cambios de contexto_ son costosos, el sistema operativo y el procesador deben trabajar juntos para:

1. Guardar los registros del procesador para restaurarlos cuando el proceso vuelva a ejecución.

2. Marcar como inválidas las entradas de caché de las tablas de página (los _TLB_, _Translation Lookaside Buffer_) y copiar las entradas modificadas a sus correspondientes entradas de las tablas de página del proceso (_TLB flushing_).

3. Restaurar los registros del procesador y tablas de páginas.

Hay costes adicionales, como los _fallos de caché_ -de memoria RAM y del _TLB_- porque el procesador accede a direcciones físicas y tablas de páginas diferentes. El coste es todavía algo superior si el proceso se ejecutó antes en un procesador diferente. El _scheduler_ está diseñado y en constante evolución para tomar en cuenta todos estos parámetros conseguir objetivos contradictorios: aumentar el rendimiento global del sistema, asegurar buenos tiempos de respuesta y evitar esperas prolongadas.

Los procesos no tienen el control del procesador ni cuándo se ejecutarán, dada la complejidad de las interacciones y eventos también es muy difícil predecir o repetir exactamente una secuencia de ejecución, por eso los _schedulers_ de sistemas operativos de uso general se dicen que son _no determinísticos_.

=== Hilos

Los procesos _tradicionales_ no comparten memoria por requisitos de seguridad y protección de memoria. Es imposible que un proceso acceda a la memoria de otros salvo que se usen mecanismos ad hoc para compartir segmentos (como el +shmget+ del estándar System V). A principios de la década de 1980 se empezaron a desarrollar programas, sobre todo interactivos, más complejos y que requerían responder a una multitud de eventos diferentesfootnote:[Por ejemplo un procesador de texto, hay que responder al teclado, otro módulo que se encarga de la paginación, otro del correcto ortográfico, etc.].

Este tipo de programación se denomina dirigida por eventos (_event driven_) donde se seleccionan los diferentes eventos dentro de un bucle y se llaman a las funciones correspondientes. La programación de este tipo es compleja para estructurar y asegurar que se ejecuta sin errores. De esta necesidad surgieron dos conceptos muy vigentes hoy y que en general se encuadran en lo que conocemos como _programación concurrente_.

Por un lado se desarrollaron librerías -sobre todo gráficas e interfaces de usuario- y lenguajes que facilitan la programación de diferentes módulos que se ejecutan independientemente de los demás. A este tipo de programación se la conoce como _programación asincrónica_.

Como una forma de facilitar aún más el desarrollo de módulos asincrónicos se desarrolló el concepto de hilos (_threads_) o _procesos ligeros_ (_light weight processes_, también llamado tareas en Ada y en sistemas de tiempo real). En vez de crear una copia de toda la imagen de memoria de un proceso cuando se crea uno nuevofootnote:[Como hace el +fork+ en Unix.] se mantiene la misma copia para ambos procesos salvo la pila (cada uno tiene su propio contexto de ejecución). Los hilos comparten el código, variables estáticas y la memoria asignada dinámicamente entre todos los creados por el mismo proceso _padre_.

Desde el punto de vista del _scheduler_ los hilos son idénticos a procesos independientes, cada uno de ellos -al igual que los procesos tradicionales- son _unidades de planificación_. Si los hilos se ejecutan en un sistema multiprocesador además de ejecutarse asincrónicamente pueden hacerlo en paralelo en diferentes procesadores. Por la popularización de los chips _multicore_ la programación con hilos se convirtió en una parte importante de la programación concurrrentefootnote:[Aunque muchos confunden la capacidad de ejecución asincrónica con paralelismo, de nuevo, el paralelismo es solo una forma de ejecución de programas concurrentes.].

Además de las facilidades que brinda a los programadores los hilos son más _baratos_ que los procesos. Consumen menos memoria y al no tener que copiar toda la memoria el tiempo de creación de nuevos hilos es mucho menor que el de procesos tradicionales. Tiene otras ventajas más sutiles, al compatir gran parte de la memoria entre os diferentes hilos el coste de los cambios de contexto es también menor, se invalidan y reemplazan menos entradas del _TLB_ y las líneas de caché.


****
Las librerías _POSIX Threads_ definen el estándar para crear y gestionar hilos en Unix. La función +pthread_create+ crea un nuevo hilo, un argumento obligatorio es la función que debe empezar a ejecutar el nuevo hilo. Cuando dicha función acabe el hilo se destruirá, aunque se puede llamar a +pthread_exit+ en cualquier punto de la ejecución.

Desde antes de la estandarización de POSIX Thread Linux ofrecía la llamada de sistema +clone+, puede crear procesos de cualquiera de los dos tipos, los tradicionales como con +fork+ o hilos similares a los creados por +pthread_create+.
****


==== Hilos ligeros
Antes de que los sistemas operativos diesen soporte estándar para la creación de hilos (como POSIX Thread en Unix o +clone+ en Linux) algunos lenguajes y máquinas virtuales los simulaban con sus propios _schedulers_ a nivel de aplicación. Los casos más conocidos son los hilos ligeros en la máquina vitual de Erlang y la antigua emulación de hilos en la máquina virtual de Java, los _green threads_.

Muchos lenguajes usan hilos ligeros para reducir el coste de la creación de hilos nativos del sistema operativo. En Go se denominan _goroutines_, permiten crear hilos con muy pocas instrucciones (aseguran que tres) y consumo de memoria de muy pocos kilobytes. En otros lenguajes se suelen llamar _tasklets_, también suelen incluir esta capacidad los módulos de _programación asíncrona_ de lenguajes dinámicos.

Hay que tener en cuenta que desde el punto de vista del sistema operativo los hilos ligeros son invisibles y por lo tanto no son planificados por el _scheduler_ sino internamente por el programa o máquina virtual. Esto implica que no pueden ejecutarse en paralelo a menos que creen hilos nativos con este propósito, como hace Gofootnote:[Lo veréis en los ejemplos de este libro en Go, se indica el número de hilos nativos a crear con la función +runtime.GOMAXPROCS+.] o Erlang desde la versión _SMP_ R11Bfootnote:[Cuando se arranca el intérprete +erl+ se pueden ver mensajes similares a `[smp:4:4] [async-threads:10]`, indica que arranca automáticamente diez hilos ligeros y cuatro nativos -detectó que el sistema tiene cuatro núcleos-.].


=== Programas concurrentes
La necesidad de programar módulos asincrónicos que respondan a los diferentes eventos y las facilidades de compartición de memoria de procesos hizo que fuese más conveniente diseñar programas como una composición de módulos, cada uno es responsable de tareas específicas. Cada módulo se ejecuta en diferentes procesosfootnote:[Salvo que sea necesario y se indique explícitamente nos referiremos en general como _procesos_ aunque estrictamente sean hilos nativos o _ligeros_, la distinción es irrelevante si la ejecución es asíncrona y no determinística.] independientes y asíncronos. Llamamos _programa concurrente_ a la composición de módulos que colaboran entre ellos.

[IMPORTANT]
.Programación concurrente
====
Es la composición de módulos que se ejecutan independientemente de forma no determinística.
====

La programación concurrente tiene ventajas pero tienen su coste. Los compartición de recursos -fundamentalmente memoria- tiene riesgos que generan nuevos problemas difíciles de detectar y analizar sin el conocimiento y herramientas adecuadas. Debido al carácter naturalmente asincrona y no determinística no podemos razonar sobe la ejecución de estos programas como una ejecución secuencial de instrucciones.

El interés de soluciones para los problemas de concurrencia no es nuevo, surgió con la aparición de los primeros _monitores_ -los predecesores de los modernos sistemas operativos- a principios de la década de 1960. De hecho, el núcleo de los sistemas operativos es una composición compleja de módulos independientes que deben responder -asincrónicamente- a una enorme diversidad de eventos (interacción con dispositivos, interrupciones de hardware, llamadas de sistema, etc.) que generaban problemas de inconsistencia en las complejas estructuras internasfootnote:[Muchas de las _pantallas azules_ y los _kernel panics_ son el resultado de problemas de concurrencia no resueltos.].

Se llamó _problemas de concurrencia_ a este tipo de errores ocasionados por el _acceso concurrente_ a recursos compartidos. El caso más habitual y más estudiado son los errores e inconsistencias generadas por el acceso no controlado a algunos recursos, es lo que conocemos como el problema de _exclusión mutua_ o _secciones críticas_. Durante décadas los problemas de concurrencia estuvieron reservados a los desarrolladores de sistemas operativos. Con la popularización de los procesadores _multicore_ se desarrollaron lenguajes y librerías que facilitaron la programación concurrente, el resultado es que la concurrencia dejó de ser esa ooscura área de conocimiento reservado a unos pocos expertos a convertirse a una necesidad de profesional para un proporción importante de programadores.

[IMPORTANT]
.Concurrencia vs paralelismo
====
El paralelismo solo es una forma de ejecutar un programa concurrente. La programación concurrente es la forma de estructurar los programas, no el número de procesadores que se usa para su ejecución.
====

Los problemas de los procesos concurrentes no están generados por los multiprocesadores o la ejecución en paralelo. Los mismos problemas ocurren con un único procesador, estos se generan no por el paralelismo sino por la intercalación de instrucciones.


=== Intercalación
En un sistema operativo moderno la ejecución secuencial de un proceso puede ser detenida en cualquier momento entre la ejecución de dos instrucciones del procesador por una interrupción de hardware. Cuando estas ocurren el procesador ejecuta una función (_interrupt handler_) predeterminada por la tabla de interrupciones del núcleo del sistema operativo. Una vez finalizado el tratamiento de dicha interrupción el _scheduler_ decide qué proceso se ejecutará a continuación. Puede elegir al mismo que estaba antes o a cualquier otro proceso de los que están _listos para ejecutar_.

En un sistema con un único procesador la ejecución de procesos es una _intercalación exclusiva_.

.Intercalado exclusivo de procesos _A_, _B_ y _C_
image::interleaving.png[height="120", align="center"]

El _scheduler_ selecciona el proceso que se ejecutará sin interrupción durante un período de tiempo denominado _ráfaga de CPU_ (_CPU burst_). La duración de la ráfaga de CPU no se puede conocer a priori, depende de muchos factores internos y externos al sistema, fundamentalmente el cuanto que le asigna el _scheduler_, llamadas de sistema del proceso y las interrupciones de dispositivos que pueden generar cambios de estado de procesos.

Las combinaciones de intercalación entre los diferentes procesos es no determinística, es altamente improbable que se pueda repetir la misma secuencia de intercalaciones entre pares de procesos. Todos los procesos comparten y compiten por recursos del sistema (procesador, memoria, acceso a dispositivos, ficheros, etc.), si estos son independientes entre ellos es el procesador y el sistema operativo los que se encargan de que se cumpla la _consistencia secuencial_ de cada uno de ellos. El programador no se tiene que preocupar de los problemas ocasionados por las intercalaciones de sus programas ni de la competencia por recursos, es responsabilidad del sistema operativo.


En un sistema con varios procesadores se produce _superposición_ de ejecuciones además de la intercalación.

.Multiprocesamiento
image::multiprocessing.png[height="120", align="center"]

Una de los objetivos de los sistemas operativos es gestionar y ocultar las complejidades de la concurrencia, así se desarrollaron mecanismos complejos para asegurar la consistencia secuencial de cada proceso individual. Pero poco pueden hacer cuando se trata de programas compuestos por módulos de ejecución independiente, la responsabilidad es también del programa.

A nivel de procesos de usuarios gestionados por sistema operativos de multiprogramaciónfootnote:[Que sí tiene la responsabilidad de gestionar múltiples procesadores.], la superposición no complica la resolución de los problemas de sincronización y acceso concurrente, la intercalación y ejecución no determinística son el origen de sus riesgos. Los algoritmos de sincronización con intercalación exclusiva también son correctos con superposición. Una solución correcta de exclusión mutua es equivalente y funciona para ambos modos de ejecución: el paralelismo es solo un caso particular de la intercalación.

****
Los estudios de concurrencia y paralelismo son diferentes. El primero se ocupa de la correcta composición de componentes no determinísticos, el segundo de la eficiencia asintótica de programas con comportamiento determinístico.
****


==== Los problemas de la intercalación
Los programadores estamos acostumbrados al modelo de consistencia secuencial de los lenguajes de programación: una instrucción que está después de otra se ejecuta ejecuta a continuación. Una de las propiedades que distingue a la programación concurrente es que esta consistencia secuencial ya no se cumplefootnote:[Más adelante, en <<barriers>>, veremos que las arquitecturas modernas de hardware tampoco aseguran por defecto la consistencia secuencial.].

****
Un programa está formado por una secuencia de operaciones atómicas ordenadas, por ejemplo +P+ por +p~0~, p~1~, p~2~+ y +Q+ por +q~0~, q~1~, q~2~+. Una ejecución válida de +P+ y +Q+ es:

+p~0~, p~1~, p~2~, q~0~, q~1~, q~2~+

o:

+q~0~, q~1~, q~2~, p~0~, p~1~, p~2~+

Para respetar la consistencia secuencial p~1~ se debe ejecutar después de p~0~ y p~2~ después de p~1~, formalmente: +p~0~ => p~1~ => p~2~+ (lo mismo para las instrucciones de +q+). La siguiente secuencia de ejecución respeta las relaciones secuenciales anteriores por lo que también es correcta y secuencialmente consistente si se analiza cada programa por separado:

+q~0~, p~0~, p~1~, q~1~, q~2~, p~2~+

Si esas instrucciones acceden o modifican variables compartidas los resultados pueden ser diferentes dependiendo de la secuencia -no determinista- de ejecución.
****

Los lenguajes de programación están diseñados para especificar y ejecutar las instrucciones secuencialmente. Tomemos la siguiente secuencia de instrucciones que se ejecutan en un programa, con las variable +a+ y +b+ inicializadas a +0+:

[source, python]
----
a = a + 1
b = b + a
print "a, b:", a, b
----

Por el modelo de consistencia secuencial es fácil deducir que el resultado de imprimir las tres variables será +1 1+. Si las dos asignaciones se repiten el resultado será +a, b: 2 3+, el siguiente +a, b: 3 6+, etc.


Supongamos que este fragmento de código se ejecuta en procesos diferentes (+P+ y +Q+) sobre un sistema con un único procesador y que tanto +a+ como +b+ son variables compartidas. Se puede producir la siguiente intercalación de las instrucciones del programa:

----
Proceso P            Proceso Q

...
a = a + 1
                     a = a + 1
                     b = b + a
                     print "a, b:", a, b
                     ...
b = b + a
print "a, b:", a, b
----


El resultado de la ejecución será:

----
a, b: 2 2
a, b: 2 4
----

Ninguno de los valores es correcto. Si se ejecuta nuevamente el resultado podría ser diferente, depende del instante y orden en que cada proceso ejecuta las instrucciones en secciones críticas del código que acceden a recursos u _objetos compartidos_. Este problema se denomina genéricamente como _condición de carrera_ (_race condition_).

Los _bugs_ causados por condiciones de carrera son difíciles de detectar, habitualmente no son frecuentes porque la probabilidad de que ocurra es bajafootnote:[Al contrario de los ejemplos en este libro, diseñados de tal manera que se aumenta artificialmente la probabilidad de que ocurran estas condiciones de carrera.] y es aún más difícil repetir el error con las mismas condiciones por la planficación de CPU no determinística.

Las dos líneas (tres contando el +print+) acceden a variables compartidas con dependencia entre ellas: el resultado de +b+ depende de +a+. Las secuencias anteriores de instrucciones no son _atómicas_, el proceso puede ser interrumpido y ejecutarse otro que modifica las mismas variables. Lo mismo puede ocurrir con instrucciones más básicas, por ejemplo con una suma:

    counter += 1

Se suele suponer que una operación tan básica como sumar una constante (o _literal_) a una variable es una operación atómica, pero no es así. El código ejecutable está compuesto por al menos tres instrucciones de procesador:

----
movl  counter(%rip), %eax
addl  $1, %eax
movl  %eax, counter(%rip)
----

Si se ejecuta dos veces el valor de +counter+ será +2+, pero es posible que se presente la siguiente condición de carrera por la intercalación de las instrucciones atómicas:

----
movl counter(%rip), %eax <1>
                    movl counter(%rip), %eax
                    addl $1, %eax
                    movl %eax, counter(%rip)
addl $1, %eax            <2>
movl %eax, counter(%rip)
----
<1> Se almacena 0 en el registro +eax+.
<2> Aunque la variable ya tiene almacenado el valor 1, el registro +eax+ sigue siendo 0.

En este caso el valor será +1+, se ha _perdido_ una operación. Es el problema más habitual. También pasa con lenguajes dinámicos y con compilación de _bytecode_ como Java o Python. El siguiente código es el generado por la compilación de Python, son cuatro instrucciones:

----
LOAD_GLOBAL   0 (counter)
LOAD_CONST    1 (1)
INPLACE_ADD
STORE_GLOBAL  0 (counter)
----

===== Ejemplos en diferentes lenguajes

Los siguientes programas  <<counter_c, en C>>, <<gocounter_go, Go>>, <<counter_java, Java>> y <<counter_py, Python>> hacen lo mismo: crean dos hilos nativos que incrementan una variable compartida (+counter+) cuyo valor al final de las ejecuciones debería ser diez millones. El resultado de una de sus ejecuciones:

[[counter_times]]
.Resultados y tiempos de CPUfootnote:[Compara los _tiempos de CPU_ con los _tiempos de reloj_. Salvo Python todos lo superan, se ejecutan en paralelo en dos CPUs por lo que por cada segundo de reloj corresponde a dos segundos de procesador. Los programas en Python no pueden ejecutarse simultáneamente en más de un procesador debido a al _Python Global Interpreter Lock_.]
----
$ time ./counter
Counter value: 5785131 Expected: 10000000
real    0m0.010s <1>
user    0m0.017s
sys     0m0.000s

$ time ./gocounter
Counter value: 5052927 Expected: 10000000
real    0m0.021s <1>
user    0m0.032s
sys     0m0.008s

$ time java Counter
Counter value: 4406963 Expected: 10000000
real    0m0.333s <1>
user    0m0.564s
sys     0m0.020s

$ time ./counter.py
Counter value: 7737979 Expected: 10000000
real    0m5.400s <2>
user    0m5.365s
sys     0m0.044s
----
<1> El tiempo de _reloj_ es menor al tiempo acumulado de CPU.
<2> El tiempo de _reloj_ es mayor al tiempo acumulado de CPU.


Se observa que en todos _perdieron_ hasta más de la mitad de los operaciones. El error se debe a la intercalación de instrucciones, éstas pueden ocurrir tanto en sistemas con un único procesador como con paralelismo. De hecho en Python no hay nada de paralelismo, el intérpete usado -CPython- crea hilos nativos pero no hay ejecución en paralelo, el _Python Global Interpreter Lock_ (<<Sampson>>) obliga a _serializar_ cada una de las instrucciones que ejecuta la máquina virtual.

Los mismos errores se obtendran si se ejecutan como hilos independientes, el siguiente es el resultado de la ejecución de <<counter_c, counter.c>> en una Rasperry 1 con un único núcleo y por lo tanto con intercalación exclusiva.

----
$ time ./counter
Counter value: 7496883 Expected: 10000000
real	0m0.353s
user	0m0.340s
sys     0m0.000s
----




////

http://talks.golang.org/2012/waza.slide#6
Concurrency
Programming as the composition of independently executing processes.
(Processes in the general sense, not Linux processes. Famously hard to define.)

Parallelism
Programming as the simultaneous execution of (possibly related) computations.

Concurrency vs. parallelism
Concurrency is about dealing with lots of things at once.
Parallelism is about doing lots of things at once.
Not the same, but related.
Concurrency is about structure, parallelism is about execution.
Concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.

Conclusion
Concurrency is powerful.
Concurrency is not parallelism.
Concurrency enables parallelism.
Concurrency makes parallelism (and scaling and everything else) easy.


https://existentialtype.wordpress.com/2011/03/17/parallelism-is-not-concurrency/
The first thing to understand is parallelism has nothing to do with concurrency.  Concurrency is concerned with nondeterministic composition of programs (or their components).  Parallelism is concerned with asymptotic efficiency of programs with deterministic behavior

////
