[[channels]]
== 9. Canales
image::jrmora/09-canales.jpg[align="center"]

Las construcciones de programación concurrente anteriores –algoritmos, _spinlocks_, semáforos y monitores– funcionan en sistemas de memoria compartida. Los canales no tienen esta restricción. Además de su capacidad de sincronización equivalente sirven para comunicación por intercambio de _mensajes_. Estos se transfieren con las operaciones atómicas _send_ y _receive_ entre procesos remitentes (_sources_) y receptores (_receivers_). Según el comportamiento de las dos operaciones se distinguen dos tipos de comunicación:

Comunicación síncrona:: En este tipo de comunicación se requiere que ambos procesos estén sincronizados (_rendevouz_). El remitente se bloquea en el _send_ hasta que el receptor ejecuta el _receive_. Y viceversa, el receptor se bloquea hasta que el remitente envía el mensaje.

Comunicación asíncrona:: Alternativamente, se puede permitir que el remitente envíe el mensaje y continúe su ejecución sin esperar a que el receptor lo reciba. La comunicación asíncrona requiere que el canal tenga un _buffer_ para almacenar los mensajes (también llamado buzón o _mailbox_). La capacidad del _buffer_ depende del canal pero siempre es finito. Si no hay receptores el _buffer_ acabará llenándose y hará que los remitentes se bloqueen.

Aunque hay sistemas con canales que admiten _multidifusión_ (_broadcast_), en general los mensajes tienen destinatarios. Hay dos formas básicas de especificarlos (_addressing_):

- Identificando explícitamente al proceso receptor (como en Erlang, se indica el _PID_ del receptor).

- Identificando al canal. En este caso el canal puede admitir solo un remitente y receptor (también llamado _pipe_, un servicio estándar en Unix), o no imponer restricciones al número de procesos que pueden enviar o recibir (como en Go).

Los canales pueden ser de tipo estático (como en Go) o de tipos dinámicos (como en Erlang). Los canales de comunicación pueden asegurar la entrega de mensajes en el mismo orden del envío (canales FIFO) o pueden entregarlos en orden arbitrario. Unos pueden asegurar la recepción de cada mensaje (_reliable_, habitual en sistemas de memoria compartida), otros pueden descartar mensajes (_best-effort_) por errores de transmisión.

=== _CSP_ (1978)

El concepto de canales como mecanismo de sincronización entre procesos fue introducido por Hoare en su artículo seminalfootnote:[De lectura muy recomendada, uno de los artículos de _Ciencias de la Computación_ más relevantes. En solo doce páginas introduce y unifica formal y elegantemente conceptos importantes que dieron origen a varios lenguajes y tecnologías innovadoras.] _Communicating Sequential Processes_ (<<Hoare>>). En él definió un modelo formal, _CSP_, para describir la interacción entre procesos genéricos independientes que no comparten memoria y cuya única forma de comunicación y sincronización es el intercambio de _mensajes_.

La entrada de un proceso es la salida de otro, ambos se ejecutan de forma asíncrona y posiblemente en paralelo pero se sincronizarán en los puntos de entrada/salida. _CSP_ define dos operadores entre procesos: +?+ para indicar la entrada de un proceso (equivalente a _receive_) y +!+ para la salida (_send_).

Ejemplos:

Leer desde el proceso _XY_ y almacenar el contenido en las variables _x, y_:

    XY?(x, y)

Enviar el contenido de las variables _x_ e _y_ al proceso _DIV_:

    DIV!(x, y)


El primer lenguaje que se desarrolló con este modelo fue occam (1983) de David May (con la colaboración de Hoare) para la empresa británica INMOS, diseñadora de los procesadores _Transputer_. Con el tiempo se desarrolló una rama de lenguajes siguiendo este modelo: _Erlang_ (Armstrong, Virding y Williams, 1986), _Newsqueak_ (Rob Pike, 1988), _Concurrent ML_ (John Reppy, 1993), _Alef_ (Phil Winterbottom, 1995) y _Limbo_ (Dorward, Pike y Winterbottom, 1996). Erlang es el más exitoso de todos ellos, sigue siendo muy usado para sistemas concurrentesfootnote:[La mayoría de los lenguajes modernos tienen algún tipo de soporte de canales o sincronización por mensaje. Si no es por una construcción sintáctica del lenguaje lo hacen vía clases o librerías].

.Erlang
****
Erlang fue diseñado en Ericcson para sus sistemas concurrentes de alta disponibilidad. No comparte estado entre los diferentes hilos de ejecución. Los canales, como en _CSP_, son la única forma de comunicación y sincronización.

La comunicación es asíncrona, los mensajes se depositan en _buzones_ desde donde son recogidos por la especificación de patrones en el receptor (similar a los _guard commands_ de Dijkstra, también parte de _CSP_). Por todas estas características se dice que Erlang sigue el modelo de _actores_ (<<Agha>>).
****

=== Canales en Go
En 2010 Google publicó la primera versión estable del lenguaje Go diseñado por Robert Griesemer, Rob Pike, y Ken Thompson. Es un lenguaje moderno, software librefootnote:[Como todos los que usé en los ejemplos de este libro.], implementa el modelo _CSP_ con canales. Sus operaciones y la creación de hilos ligeros son construcciones sintácticas. Además permite la ejecución en paralelo especificando el número de hilos nativos del sistema operativo (_threads_) que pueden crearse. Estas características favorecen la especificación compacta y legible de los algoritmos, por ello todos los ejemplos de este capítulo están en Go.


Go incluye dos mecanismos para facilitar la programación concurrente y la ejecución en paralelo:


Hilos ligeros (o _green threads_):: Las llamadas a funciones precedidas por la instrucción +go+ –llamadas _goroutines_– hacen que estas se ejecuten de forma asíncrona, como un hilo independiente. No son hilos nativos del sistema operativo sino una pequeña pila de tamaño variable gestionada y planificada (_scheduling_) internamente por las librerías _runtime_. Independientemente de las _goroutines_ también se pueden crear hilos nativos para ejecución en paralelo. El número de hilos nativos se define con +runtime.GOMAXPROCS+. La planificación y ejecución de las _goroutines_ en los diferentes hilos nativos se hace de forma automática y transparente al programador.


Canales:: Los canales son objetos de primer orden, pueden ser pasados como argumentos en funciones, _goroutines_ y hasta en mensajesfootnote:[Por ello se dice que Go también implementa el modelo _cálculo-π_.]. La implementación de canales está directamente inspirado de _CSP_ y con ideas ya usadas en Newsqueak y Limbo. Por defecto los canales son síncronos pero también pueden ser asíncronos si se especifica el tamaño de _buffer_ cuando se inicializan (con +make+). Los canales son de tipo estático, pueden ser tipos nativos o cualquier tipo definido por el programador. La operación de envío o recepción de mensajes tiene la forma +recipiente *<-* origen+, donde +recipiente+ y +origen+ pueden ser indistintamente canales o variables.

La siguiente línea crea un canal de tipo entero:

    ch := make(chan int)

El canal +ch+ tiene _buffer_ cero por lo que será un canal síncrono, si se desea un canal asíncrono hay que especificar el tamaño del _buffer_ en el segundo argumento de +make+:

    ch := make(chan int, 256)

Enviar el contenido de la variable +message+ al canal +ch+:

    ch <- message

Leer un mensaje del canal +ch+ y almacenarlo en la variable +message+:

    message = <-ch

Leer un mensaje de +ch+ y descartar su valor:

    <-ch

En los ejemplos en Go de capítulos anteriores se usó el canal +done+ para hacer que el programa principal espere por la finalización de las _goroutines_:

[source, go]
----
func run(done chan bool) {
    ...
    done <- true
}

func main() {
    done := make()
    go run(done)
    <-done
}
----

Dado que implementan variantes del modelo _CSP_ y gestionan los _hilos ligeros_ de forma muy similar, es inevitable –y habitual– la comparación entre Erlang y Go. Aunque ambos implementan el modelo _CSP_ derivan de ramas históricas diferentes. Sus diferencias claves son:

- En Erlang como en _CSP_ originalfootnote:[Aunque Hoare planteó la alternativa _atractiva_ (sic) equivalente de nombrar o etiquetar a los canales.] se especifica al proceso receptor. En Go se especifica el canal, cualquier proceso puede recibir o enviar al mismo canal.

- En Erlang se pueden enviar diferentes tipos de mensajes a cada proceso.Estos se depositan en un buzón y son recogidos según las reglas especificadas (_guard commands_) en el receptor. Los canales en Go son de tipos estáticos y la entrega de mensajes es en orden FIFO.

- Erlang sigue el modelo de _actores_, no se permite la compartición de memoria entre los diferentes hilos (_share nothing_ forzado). Aunque en Go se recomienda que toda compartición se haga mediante mensajes, es posible –a veces inevitable– compartir datos vía variables globales (como hemos visto en los ejemplos de capítulos anteriores) o incluso pasando punteros en los mensajes.

El siguiente ejemplo de Erlang define una función anónima que recibe un mensaje y lo imprime por consola. El programa crea un nuevo hilo ligero con +spawn+ y almacena su identificación en +Pid+, posteriormente le envía el mensaje +Hello+ (con el símbolo +!+ como en _CSP_ original de Hoare):

[source, erlang]
----
Pid = spawn(fun() ->
          receive Message ->
            io:format("Message: ~s", [Message])
          end
      end).

Pid ! "Hello".
----

El siguiente es el programa equivalente en Go.

[source, go]
----
channel := make(chan string)
go func() {
    fmt.Println("Message:", <-channel)
}()

channel <- "Hello"
----

Los programas son equivalentes y muy similares. Las diferencias fundamentales son la especificación del destinatario del mensaje y que en Erlang no hace falta crear canales explícitamente.

=== Barreras

Las <<sync_barrier, barreras de sincronización>> son un buen ejemplo para introducir el uso de canales como mecanismos de sincronización.

==== Barreras binarias
Una <<sync_barrier, barrera>> para dos procesos es, al igual que con semáforos, un ejemplo sencillo para implementar con mensajes. Dos procesos, _A_ y _B_, deben coordinarse. _A_ no debe pasar de un punto hasta que _B_ haya llegado, y viceversa.

La solución con semáforos requería dos, con canales es similar. La primera idea suele ser que cada proceso envíe un mensaje a su canal en cuanto llegue al punto de sincronización y a continuación espere un mensaje en el canal del otro proceso. Por ejemplo:

[source,go]
----
    ch_a = make(chan bool)
    ch_b = make(chan bool)

A                   B

...                 ...
ch_a <- true        cha_b <- true
<-ch_b              <-ch_a
...                 ...
----

El código anterior es erróneo, produce interbloqueo. El _runtime_ de Go interrumpirá el programa completo y avisará del _deadlock_.

----
fatal error: all goroutines are asleep - deadlock!
----

Es un error habitual cuando no se tiene experiencia con sincronización con canales: no tener en cuenta que por defecto ambos canales son síncronos: _A_ y _B_ se bloquean al enviar el mensaje y ninguno de ellos podrá continuar hasta que el otro haya recibido el mensaje (<<railroad_quote>>).

El interbloqueo se produce por una _espera circular_, muy similar a la que analizamos con el interbloqueo de los filósofos (<<deadlocks>>). Se puede evitar haciendo que las operaciones no sigan el mismo orden, uno de los procesos recibe primero el mensaje del otro y luego envía el propio. Por ejemplo (<<barrier_2p_sync_go, código>>):

[source,go]
----
A                   B

ch_a <- true        <-ch_a
<-ch_b              cha_b <- true
----

Para evitar las soluciones asimétricas hay que recurrir a canales asíncronos. Por defecto los canales están son síncronos pero se puede especificar el tamaño del _buffer_, en este caso es suficiente con tamaño 1 (<<barrier_2p_async_go, código>>):

[source,go]
----
    ch_a = make(chan bool, 1)
    ch_b = make(chan bool, 1)

A                   B

ch_a <- true        ch_b <- true
<-ch_b              <-ch_a
----

Como ambos canales ahora tienen _buffer_ los procesos no se bloquearán si al enviar no hay ningún proceso esperando. Desde el punto de vista de sincronización la idea es similar al valor o _número de permisos_ de los semáforos. Si un semáforo vale cero bloqueará al primer _wait_, pero si es uno el proceso que haga el primer _wait_ podrá continuar (como se hace con los semáforos usados como _mutex_).

En los ejemplos de sincronización de este capítulo –y en aplicaciones reales– es habitual recurrir a canales síncronos o asíncronos con _buffer_ de tamaño uno.

==== Barreras generales

Para este algoritmofootnote:[No sé si alguien lo diseñó o publicó antes, no lo he visto, lo escribí desde cero para este libro.] se aprovechan las dos capacidades de los mensajes: sincronización y comunicación. En los soluciones con semáforos usamos dos: uno para contabilizar los procesos que faltan por llegar a la meta y el otro para los que ya habían salido para comenzar la siguiente fase. También usaremos dos canales con el mismo objetivo, pero en lugar de variables compartidas –sujetas a los problemas de condiciones de carrera– el contador estará almacenado en un mensaje que se copiará entre los procesos: cada uno lo recogerá, actualizará y volverá a enviar (<<barrier_go, código>>).

Se requieren dos canales de tipo entero, +arrival+ y +departure+, y una variable +n+. Esta última es inmutable, se inicializa con el número de procesos a sincronizar. Definimos la estructura +Barrier+ con estos tres componentes:


[source,go]
----
type Barrier struct {
    arrival   chan int
    departure chan int
    n         int
}
----

Y una función constructora que inicializará ambos canales y el valor de +n+:

[source,go]
----
func NewBarrier(value int) *Barrier {
    b := new(Barrier)
    b.arrival = make(chan int, 1)
    b.departure = make(chan int, 1)
    b.n = value

    b.arrival <- value  <1>
    return b
}
----
<1> Se deposita un mensaje en el canal con el número de procesos que faltan por llegar.

Los dos canales tienen _buffer_ de tamaño uno pero solo uno de ellos (+arrival+) contiene inicialmente un mensaje con el número de procesos concurrentes. La función de sincronización +Barrier+ tiene dos partes bien diferenciadas:

1. Llegadas: Se opera sobre el canal +arrival+, inicialmente con un mensaje con el total de procesos que faltan por llegar. Cuando un proceso llega, recibe el mensaje, verifica el valor, si quedan procesos por llegar lo decrementa y vuelve a enviar el mensaje al mismo canal. Si es el último en llegar no depositará el mensaje en +arrival+ sino en +departure+, con el total de procesos que se sincronizan en la barrera.

2. Salidas: Los procesos que ya llegaron al final de la fase intentan leer un mensaje de +departure+ y quedarán bloqueados hasta que llegue el último. Cuando este deposite un mensaje se despertará uno de los bloqueados y verificará el valor, si quedan procesos por salir decrementará su valor y depositará nuevamente el mensaje +departure+ para que puedan continuar los demás. El último en salir enviará un mensaje a +arrival+ para que el ciclo vuelva a comenzar.


[source,go]
----
func (b *Barrier) Barrier() {
    var v int

    // Part 1
    v = <-b.arrival         <1>
    if v > 1 {
        v--
        b.arrival <- v      <2>
    } else {
        b.departure <- b.n  <3>
    }

    // Part 2
    v = <-b.departure       <4>
    if v > 1 {
        v--
        b.departure <- v    <5>
    } else {
        b.arrival <- b.n    <6>
    }
}
----
<1> Se bloquea hasta que puede leer un mensaje desde +arrival+, el mensaje contiene el número de procesos que quedan por llegar.
<2> Si todavía quedan procesos por llegar decrementa el contador y vuelve a poner el mensaje en +arrival+.
<3> Si llegaron todos, deposita un mensaje en +departure+ para que los procesos puedan empezar la siguiente fase.
<4> Quedan bloqueados hasta que el último que llegue envíe un mensaje al canal.
<5> Si todavía quedan procesos por salir (bloqueados en +departure+), decrementa el contador y vuelve a poner el mensaje.
<6> Si llegaron todos, pone el mensaje con el número inicial de procesos en el canal de llegada.

Como la recepción y envío son operaciones atómicas no hace falta recurrir a ningún método de exclusión mutua. Además, como es un único mensaje los siguientes procesos quedarán bloqueados hasta que el anterior vuelva a depositarlo. Así se asegura que no se producen condiciones de carrera como ocurre con variables compartidas (hace falta asegurar exclusión mutua explícitamente).

=== Productores-consumidores

Los canales son productores-consumidores por diseño, no hay que hacer nada especial. Los mensajes pueden ser los elementos que se añaden o quitan del _buffer_. Si el canal no tiene _buffer_ la comunicación es síncrona, los productores siempre se bloquean hasta que un consumidor esté preparado para recibir. Si por el contrario se le asigna un _buffer_ funciona exactamente como el modelo de productores-consumidores con _buffer limitado_.

La interacción es así de sencilla (<<producer_consumer_go, código>>):

[source,go]
----
    buffer := make(chan string, BufferSize)

func consumer() {
    for {
        element := <-buffer
        ...
    }
}

func producer() {
    for {
        element := produce()
        buffer <- element
    }
}
----

Si el _buffer_ del canal está lleno los productores se bloquearán hasta que los consumidores eliminen mensajes. Si está vacío los consumidores quedarán bloqueados hasta que los productores añadan nuevos elementos. Este tipo de sincronización con comunicación es muy útil. Mientras en otros lenguajes hay que implementar mecanismos basados en semáforos o monitores, en los lenguajes basados en _CSP_ es una forma natural de interacción entre procesos.

[[channels_mutex]]
=== Mutex
La implementación de _mutex_ con mensajesfootnote:[El paquete +sync+ de Go tiene una implementación +Mutex+ que es más eficiente, usa los semáforos implementados a nivel de librería en el +runtime+ (https://golang.org/src/runtime/sema.go), el lenguaje implementa su propio _scheduler_ y usa técnicas de _spin/park_ similares a los usados por los monitores en la máquina virtual de Java.] también es sencilla (<<channel_mutex_go, código>>), inicialmente se crea un canal con capacidad 1 y se deposita un mensaje vacío (no hace falta compartir datos) que representa un _permiso_ para entrar a la sección crítica.

[source,go]
----
    m := make(Mutex, 1)
    m <- Empty{}
----

En la entrada de la sección crítica se lee del canal, como hay un mensaje en el _buffer_ podrá continuar inmediatamente, el siguiente proceso se bloqueará al no tener mensaje que recibir. El proceso que sale de la sección crítica deposita nuevamente un mensaje vacío que permitirá que entre otro o desbloqueará al que esté esperando.

[source,go]
----
func Lock() {
    <-m
}

func Unlock() {
    m <- Empty{}
}
----


Los canales también bloquean si se intenta enviar un mensaje y el _buffer_ está lleno, por lo que el _mutex_ puede ser implementado a la inversa. Un mensaje representaba a un _permiso_ pero se puede hacer que este se represente por espacio libre en el _buffer_. En este caso no hace falta depositar un mensaje en la inicialización, en el _lock_ se envía un mensaje y en el _unlock_ se recibe.


[source,go]
----
    m := make(Mutex, 1)

func Lock() {
    m <- Empty{}
}

func Unlock() {
    <-m
}
----

=== Semáforos

Para semáforos generales se puede usar la misma idea que con la primera versión anterior de _mutex_ (<<channel_semaphore_go, código>>), cada mensaje representa un permiso. Solo hace falta una cola a la que hay que iniciar con tantos mensajes como el valor inicial del semáforo:

[source,go]
----
func NewSem(value int) Sem {
    s := make(Sem, 256)
    for i := 0; i < value; i++ {
        s <- Empty{}
    }
    return s
}
----

La operación _wait_ lee un mensaje y _signal_ envía uno vacío:

[source, go]
----
func (s Sem) Wait() {
    <-s
}

func (s Sem) Signal() {
    s <- Empty{}
}
----

El problema de esta solución es la dimensión del _buffer_ del canal: su tamaño debe ser igual al número máximo de permisos del semáforo (el valor máximo de su valor). De lo contrario las operaciones _signal_ también se bloquearán si está lleno. Si no se requieren valores elevados es una solución razonable, si no es así hay que buscar otra solución que no requiera que la dimensión del canal dependa del valor del semáforo.


==== Tamaño del _buffer_ independiente del valor

Una solución de este tipo requeriría, como en los algoritmos de barreras o productores-consumidores, de una cola para mantener un mensaje con el valor actual del semáforo (+value+) y otra cola para bloquear en _wait_ si el semáforo toma un valor negativo (+queue+). La solución no es muy diferente a la simulación de <<monitors_semaphores, semáforos con monitores>> o la implementación del <<futex_semaphore, semáforo con FUTEX>>. En el primer caso usamos la cola de la variable de condición para bloquear a los procesos, en el segundo la cola del FUTEX. Para la siguiente solución usamos el canal +queue+ para mantener la cola de bloqueados.

La estructura e inicialización es la siguiente (<<channel_semaphore2_go, código>>):

[source, go]
----
type Sem struct {
    value chan int
    queue chan Empty
}

func NewSem(value int) Sem {
    var s Sem
    s.value = make(chan int, 1)
    s.queue = make(chan Empty)
    s.value <- value            <1>
    return s
}
----
<1> El canal +value+ se inicializa con un mensaje que almacena el valor del semáforo.

Los algoritmos de las operaciones _wait_ y _signal_ son prácticamente idénticos a la <<semaphore_definition, definición>> de semáforos. La diferencia es que en lugar de una variable compartida usamos un mensaje para almacenar el valor.

La función +Wait+ lee el mensaje con el valor del semáforo, lo decrementa y vuelve a depositar el mensaje en el canal. Si el valor del semáforo es menor que cero se bloqueará en el canal +queue+ hasta que otro proceso ejecute +Signal+.

[source, go]
----
func (s Sem) Wait() {
    v := <-s.value
    v--
    s.value <- v
    if v < 0 {
        <-s.queue
    }
}
----

+Signal+ es la inversa, incrementa el valor del semáforo, si el resultado es menor o igual que cero hay procesos esperando un mensaje en el canal +queue+ por lo que enviará un mensaje para desbloquear al siguiente.

[source, go]
----
func (s Sem) Signal() {
    v := <-s.value
    v++
    s.value <- v
    if v <= 0 {
        s.queue <- Empty{}
    }
}
----

Puede parecer que hay riesgos de _condiciones de carrera_ porque el envío y recepción en +queue+ se hacen después de enviar el valor, pero no existe ese problema. Si al llamar a +Wait+ la variable local +v+ es menor que cero el proceso obligatoriamente debe esperar un mensaje (en +queue+). La función +Signal+ espera que se haga así y enviará siempre el mensaje correspondiente.

===== Optimización
El algoritmo puede optimizarse con una breve modificación en el canal +queue+. Si un proceso en +Wait+ ejecuta `s.value <- v` y se interrumpe, el proceso que ejecuta +Signal+ se bloqueará momentáneamente en `s.queue <- Empty{}`. El canal es síncrono por lo que no puede continuar hasta que en +Wait+ se haya ejecutado `<-s.queue`.

Se puede hacer que el canal +queue+ tenga un _buffer_ pequeño, por ejemplo `s.queue = make(chan Empty, 1)`. No cambia el algoritmo, sigue siendo correcto pero la diferencia es notablefootnote:[En el ejemplo de incrementar el contador los tiempos se reducen hasta cuatro veces.].

[[channels_philosophers_simple]]
=== Filósofos cenando
La solución natural con canales asíncronos es definir un array de canales, uno para cada tenedor (<<channel_philosophers_go, código>>). Durante la inicialización se deposita un mensaje en cada uno de ellos indicando su disponibilidad:

[source, go]
----
var forks [Philosophers]chan Empty

for i := range forks {
    forks[i] = make(chan Empty, 1)
    forks[i] <- Empty{}
}
----

Para tomar los tenedores, cada filósofo lee de los canales de cada tenedor. Si está disponible habrá un mensaje y podrá continuar, caso contrario se quedará bloqueado hasta que el tenedor sea liberado. Para evitar interbloqueos (ya analizados en la <<dining_philosophers, solución con semáforos>>) evitamos la espera circular haciendo que siempre se tome primero el tenedor con el menor identificador:


[source,go]
----
func pick(id int) {
    if id < right(id) {
        <-forks[id]
        <-forks[right(id)]
    } else {
        <-forks[right(id)]
        <-forks[id]
    }
}
----

Para liberar los tenedores es suficiente con enviar un mensaje a sus canales. Si otros filósofos están esperando se desbloquearán inmediatamente.

[source, go]
----
func release(id int) {
    forks[id] <- Empty{}
    forks[right(id)] <- Empty{}
}
----

==== Con canales síncronos

El algoritmo anterior solo funciona con canales asíncronos. En el modelo _CSP_ los canales son síncronos y Hoare propuso una solución correctafootnote:[Aunque produce interbloqueo, lo avisa en el mismo artículo.].

[[philosophers_hoare]]
.Filósofos en _CSP_
image::hoare_philosophers.png[align="center"]

La solución es más sencilla de lo que parece (<<channel_philosophers_sync_go, código>>). Hay que hacer como propuso Hoare, crear un proceso adicional para cada tenedor (+fork+). El algoritmo de los filósofos no requiere cambios. Cada proceso +fork+ no requiere de ninguna computación adicional, solo recibe y envía mensajes por su canal:

.Proceso para el tenedor _i_
[source,go]
----
func fork(i int) {
    for {
        forks[i] <- Empty{}
        <-forks[i]
    }
}
----

[NOTE]
====
Al tratarse de canales síncronos se puede invertir el orden de envío y recepción de mensajes: para tomar los tenedores los filósofos envían un mensaje y para liberarlos reciben uno. En este caso el proceso +fork+ debe invertir también sus operaciones:

[source, go]
----
for {
    forks[i] <- Empty{}
    <-forks[i]
}
----

De esta forma el programa queda idéntico a la solución propuesta por Hoare con _CSP_.
====

===== Mutex con canales síncronos
Los procesos comunicados por canales asíncronos pueden ser convertidos –tal como acabamos de hacer– a uno equivalente con canales síncronos. La solución general es añadir nuevos procesos que suplanten las capacidades de los canales con _buffer_. En el caso de los filósofos añadimos un nuevo proceso para cada tenedor para convertirlo en una comunicación entre procesos _filósofos_ y otros _tenedores_. Para el <<channel_mutex_go, código>> de simulación de _mutex_, por ejemplo, se requieren muy pocos cambios. La función _pseudo-constructora_ de +Mutex+ con canales asíncronos crea un canal con _buffer_ de tamaño uno y deposita un mensaje:


[source,go]
----
func NewMutex() Mutex {
    m := make(Mutex, 1)
    m <- Empty{}
    return m
}
----

Dado que no podemos hacerlo con canales síncronos se requiere otro proceso. Se puede hacer que el propio constructor inicie el nuevo proceso sin necesidad de modificar la implementación de las otras funciones (<<channel_mutex_sync_go, código completo>>)footnote:[Uso función anónima con clausura, de lectura y comprensión más sencilla.]:

[source,go]
----
func NewMutex() Mutex {
    m := make(Mutex)
    go func() {         <1>
        for {
            m <- Empty{}
            <-m
        }
    }()
    return m
}
----
<1> Se lanza una _goroutine_, la función es anónima y aprovecha de la clausura para hacer referencia al mismo canal +m+.

==== Solución óptima
La solución anterior (ya la analizamos <<dining_philosophers_semaphores, con semáforos>>) no asegura que puedan comer todos los filósofos que podrían hacerlo. Se puede implementar una solución óptima similar a la de semáforos pero adaptada a canales (<<channel_philosophers_provider_go, código completo>>).

En vez de solicitar los tenedores individualmente habrá un proceso _proveedor_ (+provider+) para toda la mesa, este proceso usará un único canal síncrono para recibir los mensajes de todos los filósofos. Estos enviarán mensajes indicando si quieren tomar o soltar los tenedores. El proveedor verificará el estado de los filósofos vecinos, si ambos tenedores están libres le responderá con un mensaje para que continúe. Si alguno de sus vecinos está comiendo le responderá cuando estos hayan dejado de comer.

El mensaje de filósofos al proveedor será una estructura que indica el índice del filósofo, el estado (+Hungry+ si desea comer y +Thinking+ si es para liberar los tenedores) y el canal individual del filósofo (también síncrono) para recibir la respuestafootnote:[Go permite enviar descriptores de canales en los mensajes por lo que no hace falta que estos sean parte del estado global, cada filósofo crea el suyo y lo pasa al proveedor en el mensaje.]:

[source, go]
----
type Request struct {
    id     int
    status int
    c      chan Empty
}
----

Cuando un filósofo desea comer envía un mensaje al canal del proveedor con su identificación (+i+), su canal (+myCh+) y el estado +Hungry+. A continuación espera la respuesta del proveedor:

[source, go]
----
provider <- Request{id: i, c: myCh, status: Hungry}

<-myCh
----

Cuando libera los tenedores envía otro mensaje similar pero con el estado +Thinking+:

[source, go]
----
provider <- Request{id: i, c: myCh, status: Thinking}
----

El proveedor mantiene un array que con el estado de los filósofos y su canal de comunicación. Inicialmente cada posición es una copia de la estructura +Request+ de los mensajes. El proceso está en un bucle infinito recibiendo mensajes desde su canal +provider+. Cuando recibe uno lo copia al array de estados y verifica el estado del mensaje que acaba de recibir:

1. Si es +Hungry+ llama a la función +canEat+, esta función responderá con un mensaje al canal del filósofo si puede comer.

2. Si el estado es +Thinking+ significa que deja los tenedores por lo que llama a la función +canEat+, una vez para cada vecino que está en estado +Hungry+.

[source, go]
----
for {
    m := <-provider
    philo[m.id] = m
    switch m.status {
    case Hungry:
        canEat(m.id)
    case Thinking:
        canEat(left(m.id))
        canEat(right(m.id))
    }
}
----

La función +canEat+ es idéntica a la homónima de la solución óptima con semáforosfootnote:[Nuevamente aparecen las similitudes de sincronización entre semáforos y canales.] (<<philosophers_2_py, código Python>>), solo que en vez de señalizar un semáforo se responde con un mensaje. La función verifica el estado de los vecinos a izquierda y derecha del filósofo indicado en el argumento (+i+), si ninguno de los vecinos está comiendo entonces permite continuar enviando un mensaje al canal correspondiente.

[source, go]
----
func canEat(i int) {
    r := right(i)
    l := left(i)
    if philo[i].status == Hungry &&
        philo[l].status != Eating &&
        philo[r].status != Eating {
        philo[i].status = Eating
        philo[i].c <- Empty{}
    }
}
----

=== Paralelismo
En 1979, poco después de la publicación del artículo del modelo _CSP_, la empresa británica INMOfootnote:[Actualmente STMicroelectronics, http://www.st.com/.] pidió colaboración a Hoare para crear el lenguaje occam para su nueva arquitectura de multiprocesamiento masivo _Transputer_. A principios de la década de 1980 se pensaba que se había llegado al límite de la capacidad de los procesadoresfootnote:[Podían poner más transistores en un chip pero no sabían qué hacer con ellos, luego surgieron las arquitecturas _superescalares_ que permitieron aumentar la potencia de cálculo, lo que también significó la decadencia de _Transputer_.] por lo que diseñaron una arquitectura basada en el modelo _CSP_.

La arquitectura de _Transputer_ consistía de procesadores con instrucciones genéricas, 4 KB de RAM incluidas en el chip y cuatro puertos series de alta velocidad. Cada puerto podía usarse para conectar a otros procesadores y así formar arrays de procesasores con canales síncronosfootnote:[Llegaron a fabricar un _switch_ de red de 32x32 procesadores.].


[[BOO42]]
.Placa con Transputer con matriz de 6x7 procesadoresfootnote:[De la página David May, uno de los arquitectos de Transputer, https://www.cs.bris.ac.uk/~dave/transputer.html]
image::B0042.jpg[width="300", align="center"]

Inicialmente solo se podía programar en occam pero luego se adaptaron librerías para lenguajes como Pascal, C y Fortran. También se desarrollaron y portaron varios sistemas operativos como _Minix_, _Paros_ y _Trollius_. Aunque inicialmente tuvo éxito en el ambiente académico (ofrecía buena potencia de cálculo, sobre todo de matrices) y se usó en sistemas satelitales, posteriormente desapareció, posiblemente por la aparición de microprocesadores más potentes y económicos. O, como asegura David May –uno sus fundadores–, por el desconocimiento general de concurrencia de los programadores de la época.

Aunque ya no existe, su arquitectura influyó notablemente en el desarrollo de los chips para tratamiento digital de señales, la supercomputación basada en _clusters_ y hasta la conocida _Blue Gene_ de IBM que soporta miles de procesadores conectados por canales de alta velocidadfootnote:[Está basada en la arquitectura QCDOC, originalmente soportaba canales de comunicacion con 12 nodos vecinos y hasta 12 Gbits/seg.]


==== Multiplicación de matrices en paralelo

Una muestra de la potencia del modelo _CSP_ en arquitecturas con múltiples procesadores es el producto de matrices. Aunque el siguiente ejemplo trata con matrices y enteros pequeños, su uso estaba orientado a matrices de grandes dimensiones que compensen la sobrecarga y demoras provocados por el envío de mensajes.

Analizaremos el algoritmo para multiplicar en paralelo dos matrices de 3x3, como las de la siguiente imagen:

[[matrix_multiplication]]
image::matrix_multiplication.png[align="center"]

Cada elemento de la matriz resultante puede ser calculado independientemente. Por ejemplo, el elemento central de la matriz (25) se calcula de la siguiente forma:

[[element_multiplication]]
image::element_multiplication.png[align="center"]

El cálculo se puede descomponer en diferentes procesos _multiplicadores_ comunicados por canales. Cada uno de ellos multiplican un elemento de cada matriz, lo añaden a la suma parcial recibida desde otro proceso y envían el resultado al siguiente multiplicador. Para matrices de 3x3 se necesitan tres procesos por fila inicializados con los valores de una fila de la primera matriz ([4, 5, 6]). Del canal _norte_ (_north_)footnote:[Recordad que cada procesador de _Transputer_ tiene cuatro puertos, para ubicarlos en el diagrama los llamamos _norte_, _este_, _sur_ y _oeste_.] reciben un elemento de la fila correspondiente a la segunda matriz ([2, 1, 2]):

[[col_row_multiplication]]
image::col_row_multiplication.png[align="center"]

Para obtener el resultado final en el procesador de la columna izquierda cada proceso multiplica el valor inicial por el que le llegó desde el _norte_, lo suma al resultado desde el canal del _este_ y lo envía en su canal del _oeste_. El proceso _zero_ de la columna de la derecha únicamente envía ceros para iniciar la suma parcial, así el algoritmo de los multiplicadores es el mismo para todos:

[source, go]
----
second := <-north
sum := <-east
west <- sum + first*second
----

Tal como ya había descrito Hoare, el procedimiento anterior se puede generalizar para la multiplicación en paralelo de la matriz completa con nueve _multiplicadores_ (en el centro de la imagen). Los procesos de la fila superior envían los valores, uno a uno, de las filas de la segunda matriz, los resultados parciales lo obtienen los procesos de la columna izquierda (_result_). Cada multiplicador copia el mensaje recibido del canal _norte_ al canal _sur_ para que procesos de las siguientes filas (se añaden los procesos _sink_ de la fila inferior con el único objetivo de que el algoritmo sea el mismo para todos los multiplicadores).


[[parallel_multiplication]]
.Array de procesos para multiplicación de matrices
image::parallel_multiplication.png[align="center"]

Los algoritmo de los cuatro tipos de procesos de la matriz son los siguientes (<<parallel_matrix_multiplication_go, programa completo>>):

[source, go]
----
func multiplier(first int) {
    for {
        second := <-north
        south <- second
        sum := <-east
        west <- sum + first*second
    }
}

func result(rowNum int) {
    for i := 0; i < Dim; i++ {
        row[i] := <-east
    }
}

func source(row Row) {
    for i := range row {
        south <- row[i]
    }
}

func zero(west chan int) {
    for {
        west <- 0
    }
}

func sink() {
    for {
        <-north
    }
}
----

=== Algoritmos distribuidos

No es el objetivo de este libro, se necesitaría uno específico y bastante extenso dado el avance y cantidad de sistemas y protocolos que se desarrollaron en los últimos años. Pero no podía dejar de mencionarlo, los canales de comunicación y el modelo de _procesos comunicados_ son elementos fundamentales de los sistemas distribuidos. A estos se les añade  un tercer elemento: los _nodos_, ordenadores independientes conectados solo por un canal de comunicaciónfootnote:[De diferentes características, fundamentalmente si son fiables y entregan los mensajes en el mismo orden en que lo reciben.] (_débilmente acoplados_) y pueden ejecutar más de un proceso.


En sistemas distribuidos hay que tener en cuenta otros requerimientos y problemas que no existen en procesos concurrentes en memoria compartida:

- Los canales y nodos pueden fallar sin notificar a los demás procesos por lo que hay que considerar tiempos y caducidad.

- El grafo o estructura de la red de nodos puede ser variable, compleja y no permitir la conexión de cada nodo con todos los demás.

- No se pueden tomar decisiones suponiendo un número fijo de nodos o procesos y que cada uno de ellos recibió cada mensaje, se requieren pasos adicionales de sincronización y verificación.

- La operación que más tiempo toma es la copia de mensajes de un nodo a otro por lo que la prioridad es reducir el tamaño y número de mensajes.

==== Estructura de procesos distribuidos
Los procesos distribuidos deben responder a mensajes de sincronización que llegan desde otros nodos, lo habitual es implementar al menos un hilo auxiliar independiente responsable de recibir los mensajes de la red y responder adecuadamente lo antes posible. Un proceso que se ejecuta en un nodo (_Process_) consiste de un hilo principal (_Main_) y un auxiliar (_Receiver_) que comparten memoria y se sincronizan entre ellos con cualquiera de los mecanismos de memoria compartida.

[[distributed_process]]
image::distributed_process.png[align="center"]

Los programas diseñados según los principios de _CSP_ pueden ser fácilmente adaptados a sistemas distribuidos cambiando las primitivas _send_ y _receive_ de canales locales por sistemas de gestión de _colas de mensajes_ (como Beanstalkd o RabbitMQ). Por el mismo principio, algoritmos diseñados para sistemas distribuidos pueden ser fácilmente implementados y simulados localmente con el modelo _CSP_.

===== Simulación en Go
Los siguientes ejemplos de exclusión mutua simulan un sistema distribuido. Cada nodo es una _goroutine_ (el hilo _main_) que a su vez pone en marcha otra _goroutine_ (_receiver_) con la que comparte memoria. El patrón de los programas es el siguiente:

----
func node(aChannel chan Struct) {
    number := 0
    mutex := new(sync.Mutex)

    receiver := func() {
        for {
            request := <-aChannel
            // ...
            aChannel <- response()
            }
        }
    }
    go receiver()
    mainProcessing()
}

func main() {
    //...
    go node(aChannel)
}
----

Desde el programa principal se pone en marcha un _nodo_ llamando a la función +node+, el núcleo del proceso principal que hace el _trabajo real_. En ella se definen las variables compartidas necesarias y pone en marcha el hilo  +receiver+. En Go es una _clausura_, las variables definidas en +node+ son accesibles desde la función anónima de +receiver+.


==== Exclusión mutua distribuida
Como breve introducción al diseño de algoritmos distribuidos analizaremos uno de los algoritmos más conocidos, el de exclusión mutua distribuida por _autorización_ de Ricart-Agrawala (<<Ricart>>, 1981) basado en el conocido algoritmo de la panadería.

Al tratarse de exclusión mutua usamos las funciones +Lock+ y +Unlock+, son el pre y posprotocolo de la exclusión mutua distribuida. Como en los ejemplos previos, el programa incrementa la variable compartida +counter+ (no tiene sentido ni es posible en un sistema distribuido real pero nos sirve para verificar el funcionamiento). No es usual que se requieran secciones críticas globales en un sistema distribuido, pero la relativa simplicidad del modelo y los algoritmos son útiles para una rápida introducción a la _sensación_ de diseñar algoritmos distribuidosfootnote:[Por otro lado, un área apasionante.].

===== Algoritmo de Ricart-Agrawala (1981)

Es uno de los algoritmos distribuidos más sencillos de interpretar, el proceso que desea entrar a la sección crítica debe recibir la autorización de todos los demás (<<distributed_me1_go, código fuente>>). Para ello envía un mensaje a los demás y espera la respuesta de cada uno (cada entrada requiere _2(n-1)_ mensajes). Estos solo responderán si no hay competencia y no están en la sección crítica, o el número del remitente es menor. Como en el algoritmo de la panadería, el turno de entrada se asigna por un número creciente.

Cada nodo mantiene la siguiente información:

- el máximo número que recibió desde la red (+highestNum+);
- el número que selecciona cuando desea entrar a la sección crítica (+myNumber+);
- una cola de las respuestas pendientes a otros procesos que desean entrar (+deferred+);
- una variable booleana para indicar que el proceso está esperando para entrar a la sección crítica (+requestCS+) y
- un _mutex_ para la sincronización entre el proceso principal y +receiver+.

.Variables
[source, go]
----
highestNum := 0
myNumber := 0
deferred := make(chan int, Nodes)
requestCS := false
mutex := new(sync.Mutex)
----

En +Lock+ se indica que se quiere entrar a la sección crítica (lo necesita el hilo +receiver+), se selecciona el número (igual al más alto visto más uno) y se envía un mensaje a todos los demás procesos con el identificador y número seleccionado. Luego se espera a recibir la respuesta de todos, cuando lleguen todas el proceso estará en la sección crítica.

.Lock
[source, go]
----
mutex.Lock()            <1>
requestCS = true
myNumber = highestNum + 1
mutex.Unlock()          <1>

for i := range requests {
    if i == id {
        continue
    }
    requests[i] <- Message{source: id, number: myNumber}
}

for i := 0; i < Nodes-1; i++ {
    <-replies[id]
}
----
<1> Hay que asegurar exclusión mutua para evitar condiciones de carreras con el hilo de +receiver+.

La tarea fundamental de +Unlock+ es enviar una respuesta a todos los procesos que enviaron solicitudes (las recibió el hilo +receiver+) mientras se estaba en la sección crítica.

.Unlock
[source, go]
----
requestCS = false
mutex.Lock()            <1>
n := len(deferred)
mutex.Unlock()          <1>
for i := 0; i < n; i++ {
    src := <-deferred   <2>
    replies[src] <- Message{source: id}
}
----
<1> Hay que asegurar exclusión mutua para evitar condiciones de carreras con +receiver+.
<2> Envía la respuesta a los que están pendientes de respuesta. Fueron añadidos a +deferred+ por +receiver+.

El hilo +receiver+ se ejecuta de manera asíncrona esperando peticiones de los otros procesos, los mensajes incluyen el identificador del proceso y el número que seleccionaron (con en la panadería, puede haber números repetidos). Cuando recibe una petición responde inmediatamente si el proceso local no desea entrar a la sección crítica o el número del proceso remoto es menor. En caso contrario agrega el identificador del proceso remoto a la cola +deferred+ para que se le envíe la respuesta desde +Unlock+.

._Receiver_
[source, go]
----
for {
    m := <-requests[id]
    mutex.Lock()
    if m.number > highestNum {  <1>
        highestNum = m.number
    }
    if !requestCS ||
        (m.number < myNumber || <2>
        (m.number == myNumber &&
            m.source < id)) {   <3>
        mutex.Unlock()
        replies[m.source] <- Message{source: id}
    } else {
        deferred <- m.source    <4>
        mutex.Unlock()
    }
}
----
<1> Actualiza +highestNum+ si el número recibido de otro proceso es mayor.
<2> La comparación es similar a la del <<bakery, algoritmo de la panadería>>.
<3> Si el proceso no desea entrar a la sección crítica o el número del otro proceso es menor envía la respuesta inmediatamente.
<4> Si no, agrega el proceso a los _retrasados_ para que se envíe la respuesta después de salir de la sección crítica.

===== Algoritmos basados en paso de testigo

El algoritmo anterior no es el único ni el más eficiente. También se desarrollaron otros que minimizan la cantidad de mensajes. Dos de los más estudiados son el de paso de testigos (_token-passing_) de Ricart-Agrawala (<<Agrawala>>, <<Carvalho>>) y el de Neilsen-Mizuno (<<Neilsen>>). Los algoritmos de paso de testigo requieren +n+ mensajes cada vez que se solicita el testigo, es una reducción importante. Además, si el proceso que desea entrar ya tiene el testigo no hace falta que vuelva a solicitarlo: no solo decrementa el número de mensajes, también reduce notablemente las demoras en la entrada.


====== _Token-passing_ de Ricart-Agrawala (1983)
Este algoritmo de paso de testigo reduce considerablemente el número de mensajes (<<distributed_me2_go, código fuente>>). Para acceder a la sección crítica el proceso debe poseer el testigo (_token_), solo uno de ellos puede tenerlo. Si el proceso que desea entrar a la sección crítica no lo posee debe solicitarlo enviando una solicitud a todos los demás. El que tenga el testigo se lo pasará cuando salga de su sección crítica.

La elección de a quién le corresponde el testigo también se hace por el número elegido por cada proceso pero a diferencia del anterior no se usa un número único: cada proceso mantiene un par de arrays con los números de todos los demás. El primero (+requested+) es el número con el que solicitó el testigo cada proceso. El segundo (+granted+) el número con que se le otorgó el testigo por última vez a cada proceso. Para elegir al siguiente se  se selecciona uno cuyo número de solicitud (en +requested+) sea mayor al número de la última vez que se le otorgó el testigo (en +granted+).

Cuando se pasa el testigo de un proceso a otro también se envía el array +granted+, así se asegura que el que toma la decisión tiene la versión actualizada. El tamaño de ambos arrays es proporcional al número de nodos, es un problema para grandes redes por el espacio de almacenamiento en cada nodo como por el tamaño del mensaje cuando se transfiere el testigo.


====== _Token-passing_ de Neilsen-Mizuno (1991)
Elimina el problema de almacenar y transferir el array (<<distributed_me3_go, código fuente>>). Cada nodo mantiene solo dos variables enteras, el _padre_ (+parent+) del proceso y el identificador del siguiente nodo al que le corresponde el testigo (+deferred+).

El algoritmo se basa en la creación de árboles virtuales, +parent+ indica cuál es el padre de un proceso (así se define un árbol virtual). Inicialmente hay que asignar un padre a cada nodo para definir un árbol de cobertura (_spanning tree_) virtual, en el código de ejemplo todos se hacen hijos del proceso 0.

Cuando un proceso solicita el testigo envía un mensaje a su padre e inmediatamente se _desconecta_ del árbol (formará otro nuevo) poniendo su +parent+ en -1. Si el receptor del mensaje no tiene el testigo envía una copia del mensaje a su padre y selecciona al remitente anterior como su nuevo padre.

Supongamos que _A_ solicita el testigo y que lo tiene _D_. La situación inicial es:

[quote]
+_A_ => _B_ => _C_ => *_D_* <= _E_+

Cuando _B_ recibe el mensaje desde _A_ lo reenvía a _C_ y cambia su padre a _A_:

[quote]
+_A_ <= _B_   _C_ => *_D_* <= _E_+

El mensaje es así copiado hasta que llega a la raíz del árbol actual ligado al poseedor del testigo (_D_). Las conexiones en ese momento serán las siguientes (hay dos árboles, la raíz de uno es el poseedor el testigo, el otro es el siguiente):

[quote]
+_A_ <= _B_ <= _C_   *_D_* <= _E_+


El proceso _D_ puede estar en dos estados:

1. Si no está en la sección crítica transfiere el testigo inmediatamente al proceso original que lo solicitó.

2. Si está en la sección crítica pone al remitente del mensaje original en su +deferred+, será al que pase el testigo cuando haya salido de la sección crítica.

En cualquier de los dos casos, el árbol se habrá unificado.

[quote]
+*_A_* <= _B_ <= _C_ <= _D_ <= _E_+

El algoritmo de Neilsen-Mizuno es muy abstracto y difícil de entenderlo inicialmente, pero su programación es muy sencilla y, como veremos más adelante, también muy eficiente: compite en eficiencia con los algoritmos de memoria compartida.

Es notable como la abstracción de _árboles virtuales_, representados solo por una variable en cada nodo, reduce la complejidad e información que hay que transmitir. Este tipo de técnicas son muy comunes en algoritmos distribuidos. Me pareció importante explicarlas; los mismos conceptos e ideas pueden ser usados para programas concurrentes, sobre todo si se usan canales y se pretende no compartir memoria (_share nothing_).

[[channels_times]]
=== Eficiencia de Canales

La comparación de métodos de sincronización disímiles en lenguajes diferentes es complicado y no suelen ser justos. En el caso de Go es peor, si cabe. A diferencia de C o Java usadas en comparaciones anteriores, Go crea hilos ligeros y se planifican con el _scheduler_ interno de las librerías _runtime_.

Los canales, en principio, tienen un mayor coste que los semáforos y monitores. Además de sincronizacón sirven para comunicación, lo que requiere copiar zonas de memoria atómicamente. Podemos verificar si este sobrecoste, como se afirma a menudo, es del todo cierto. Quizás haya sorpresas.

==== Exclusión mutua

El siguiente gráfico es la comparación de mecanismos de exclusión mutua, similar al de <<monitor_times_em, monitores>> y en el mismo ordenador. Se muestran de izquierda a derecha los tiempos de retorno (en segundos) para el contador con el: _mutex_ de POSIX Threads, monitor nativo en Java, el _mutex_ del módulo +sync+ de Go y la implementación de _mutex_ con mensajesfootnote:[El gŕafico de estas pruebas es sobre un ARM y no sobre el procesador i5-2520M como las demás. Detecté que las optimizaciones del _mutex_ nativo funcionan mal en este procesador y en un i7-4770K que hacía que la emulación con mensajes obtuviese tiempos mejores. Con decenas de pruebas en procesadores diferentes, solo en esos dos se encontró el problema.].

////
.Tiempos de ejecución de los diferentes mecanismos de exclusión mutua, Intel
[caption=""]
image::channels_mutex.png[align="center"]

La primera sorpresa: la solución de _mutex_ con mensajes en un Intel i5 con cuatro núcleos es más eficiente que la _nativa_ del módulo +sync+. Los resultados son consistentes con todas las ejecuciones.

Como parece más una anomalía ejecuté los mismos programas en una Raspberry 2 (ARMv7 con dos procesadores). Los resultados son diferentes:

[NOTE]
====
En un momento pensé en cambiar el gráfico y poner resultados con otros procesadores, así no necesitaría explicar y matizar con más gráficos. Preferí ser honesto y riguroso: los datos son reales y repetibles. En ninguna de las centenares de pruebas dio un resultado diferente.

El ordenador que generó esos resultados _anómalos_ es un _Thinkpad X1_ con procesador _Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz_, el sistema operativo es Ubuntu 15.04 y la versión 1.3.3 de Go. En Github https://github.com/gallir/concurrencia_source_samples/blob/master/measurements/logs/measures_mutex_go.log[está disponible] el registro de las pruebas usadas para el gráfico. Poco después otra persona obtuvo resultados similares con un i7-4770K y Go 1.4.2 (https://github.com/gallir/concurrencia_source_samples/blob/master/measurements/logs/i7-4770K-go_mutex.png[imagen en Github]).
====

////


.Tiempos de ejecución de los diferentes mecanismos de exclusión mutua, ARMv7
[caption=""]
image::channels_mutex_arm.png[align="center"]

El _mutex_ nativo de Go tiene tiempos similares a los de POSIX Threads y Java mientras que la emulación con mensajes es considerablemente menos eficiente. Pruebas con otros procesadores dieron resultados similares, la emulación de _mutex_ con mensajes es entre 5 y 200 % más lento que el del modulo +sync+.


==== Barreras

El siguiente gráfico es también similar a la de <<barriers_monitor_java, monitores>>. Esta vez con semáforos y variables de condición de POSIX Threads, monitor de Java y canales en Go.

.Tiempos de ejecución de barreras, Intel
[caption=""]
image::channels_barrier.png[align="center"]

El tiempo de ejecución de Go es considerablemente inferior que los demás. Es sorprendente porque el contador de procesos se copia con el mensaje, no es una variable estática como en semáforos o Java. El patrón se repite en diferentes procesadores y arquitecturas.

////
Por ejemplo en ARMv7 de Raspberry 2:

.Tiempos de ejecución de barreras, ARMv7
[caption=""]
image::channels_barrier_arm.png[align="center"]
////


==== Filósofos

El siguiente es el gráfico de tiempos CPU y retorno del algoritmo de filósofos similar a la <<monitor_philosophers, comparativa en monitores>>. Se comparan la solución con <<monitor_philosophers, monitores en Java>> y las dos con canales de este capítulo: el más simple pero que no es óptimo y el último con el _proveedor_ de tenedores.

.Tiempos de ejecución de filósofos
[caption=""]
image::channels_philosophers.png[align="center"]

El menos eficiente es el del _proveedor_, tiene lógica porque la asignación de tenedores está centralizado en un único hilo, con mucha competencia y procesos se convierte en el cuello de botella. El <<channels_philosophers_simple, primer algoritmo>> es el más eficiente. Con cinco filósofos da mejores tiempos que el monitor en Java, pero con más procesos se comporta peor:

.Tiempos de CPU de 5 a 100 filósofos
[caption=""]
image::channels_philosophers_100.png[align="center"]


==== Exclusión mutua distribuida

Como curiosidad final, los tiempos de ejecución (de reloj) en el mismo ordenador del contador <<go_mutex_go, usando _mutex_>> y los algoritmos distribuidosfootnote:[Recordad que estamos simulando la _distribución_, todos los procesos y canales son locales.] de exclusión mutua.

[[distributed_comparison]]
.Tiempos de ejecución de los algoritmos de EM distribuida
image::distributed_comparison.png[align="center"]

La sobrecarga por la número de mensajes que se envían en el algoritmo de _autorización_ de Ricart-Agrawala es enorme. Los algoritmos de _token passing_ se comportan muy bien. El de Neilsen-Mizuno solo es hasta un 50 % más lento que el _mutex_ nativo. Es un dato sorprendentemente bueno considerando que se crean el doble de hilos, la lógica más compleja y que la información se copia por mensajesfootnote:[También puede indicar que la implementación de mensajes y el _scheduling_ del _runtime_ de Go es muy eficiente.].

=== Recapitulación

La popularización de ordenadores con número masivo de procesadores, servicios en la _nube_, _microservicios_, plataformas para programación distribuida y tolerante a fallos, y hasta nuevos lenguajes de programación que lo incluyen como construcción sintáctica hace que el modelo de sincronización y comunicación con canales esté de moda. Pero pocos desarrolladores conocen sus orígenes (el modelo _CSP_) y los mecanismos básicos de sincronización sobre el que se construyen algoritmos más complejos. El objetivo de este capítulo fue llenar este hueco y poner en contexto la historia y equivalencia de canales con los demás mecanismos de sincronización de procesos.

Con canales se pueden resolver los mismos problemas de concurrencia que resolvimos con semáforos y monitores. En general los tres son equivalente como mecanismos de sincronización en sistemas de memoria compartido, si se tiene uno se pueden implementar los otros (con mayor o menor dificultad).

A diferencia de los semáforos y monitores, los canales tienen la capacidad adicional de servir para la comunicación entre procesos y pueden ser usados para procesos sin memoria compartida. Esto implica que también son útiles para procesos distribuidos en diferentes nodos, la breve introducción a algoritmos distribuidos fue una muestra de esta capacidad.




////
http://www.slideshare.net/dabeaz/an-introduction-to-python-concurrency (para ver lo de mensajes)
////
