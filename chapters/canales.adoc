[[channels]]
== Canales

Las construcciones de programación concurrente de capítulos anteriores -algoritmos, _spinlocks_, semáforos y monitores- requieren y solo sirven para sistemas de memoria compartida. Los canales no tienen esta restricción, además de sus capacidad de sincronización equivalente sirven para comunicación mediante intercambio de _mensajes_. Estos se transfieren con las operaciones atómicas _send_ y _receive_ entre procesos remitentes (_sources_) y receptores (_receivers_). Según el comportamiento de las operaciones las dos operaciones se definen dos tipos de comunicaciones:

Comunicación sincrónica:: En este tipo de comunicación se requiere que ambos procesos estén sincronizados (_rendevouz_), el remitente se bloquea en el _send_ hasta que el receptor ejecute el _receive_. Y viceversa, el receptor se bloquea hasta que el remitente envíe el mensaje.

Comunicación asincrónica:: Alternativamente se puede permitir que el remitente envíe el mensaje y continúe su ejecución sin esperar a que el receptor lo reciba. La comunicación asincrónica requiere que el canal tenga un _buffer_ para almacenar los mensajes (también llamado buzón o _mailbox_). La capacidad del _buffer_ depende del canal, si no hay receptores o estos consumen mensajes a menor ritmo el _buffer_ acabará llenándose y hará que los remitentes se bloqueen.

Aunque hay sistemas con canales que admiten _multidifusión_ (_broadcast_) en general los mensajes tienen destinatarios, hay dos formas básicas de especificarlos (_addressing_):

- Identificando explícitamente al proceso receptor, como en Erlang donde se indica el _PID_ del receptor.

- Identificando al canal. En estos casos el canal puede admitir solo un remitente y receptor (también llamado _pipe_, un servicio estándar en Unix) o no poner restricciones al número de procesos que pueden enviar o recibir (como en Go).

Los canales pueden ser de tipo estático (como en Go) o de tipos dinámicos (como en Erlang). Los canales de comunicación pueden asegurar la entrega de mensajes en mismo orden de envío (canales FIFO) o pueden entregarlos en orden arbitrario. Unos pueden asegurar la recepción de cada mensaje (_reliable_, lo habitual en sistemas de memoria compartida), otros pueden descartar mensajes (_best-effort_) por errores de transmisión.

=== _CSP_

El concepto de canales como mecanismo de sincronización entre procesos fue introducido por Hoare en su artículo seminalfootnote:[De lectura muy recomendada, uno de los artículos de _ciencias de la computación_ más relevantes. En solo doce páginas introduce y unifica formal y elegantemente conceptos importantes que dieron origen a varios lenguajes y tecnologías innovadoras.] _Communicating Sequential Processes_ (<<Hoare>>). En él definió un modelo formal, _CSP_, para describir la interacción entre procesos genéricos independientes que no comparten memoria y cuya única forma de comunicación y sincronización en el intercambio de _mensajes_. La entrada de un proceso es la salida de otro proceso, ambos procesos se ejecutan asincrónicamente y posiblemente en paralelo pero se sincronizarán en los puntos de entrada/salida. _CSP_ define dos operadores entre procesos, +?+ para indicar la entrada de un proceso (equivalente a _receive_) y +!+ para la salida (_send_).

Ejemplos:

Leer desde el proceso _XY_ y almacenar el contenido en las variables _x, y_:

    XY?(x, y)

Enviar el contenido de _x_ e _y_ al proceso _DIV_:

    DIV!(x, y)


El primer lenguaje que se desarrolló con este modelo fue occam (1983) de David May (con la colaboración de Hoare) para la empresa británica INMOS, diseñadora de los procesadores _Transputer_. Con el tiempo de diseñaron una rama de lenguajes siguiendo este modelo: Erlang (Armstrong, Virding y Williams, 1986), Newsqueak (Rob Pike, 1988), Concurrent ML (John Reppy, 1993),  Alef (Phil Winterbottom, 1995) y Limbo (Dorward, Pike y Winterbottom, 1996). Erlang es el más exitoso de todos ellos, sigue siendo muy usado para sistemas concurrentesfootnote:[La mayoría de los lenguajes modernos tienen algún tipo de soporte de canales o sincronización por mensaje, si no es una construcción sintáctica del lenguaje lo hacen vía clases o librerías].

.Erlang
****
Erlang fue diseñado en Ericcson para sus sistemas concurrentes de alta disponibilidad. No comparte estado entre los diferentes hilos de ejecución. Los canales, como en _CSP_, son la única forma de comunicación y sincronización. La comunicación es asincrónica, los mensajes se depositan en _buzones_ desde donde son recogidos por la especificación de patrones en el receptor (similar a los _guard commands_ de Dijkstra, también parte de _CSP_). Por todas estas características se dice que Erlang sigue el modelo de _actores_ (<<Agha>>).
****

=== Canales en Go
En 2010 Google publicó la primera versión estable del lenguaje Go diseñado por Robert Griesemer, Rob Pike, y Ken Thompson. Es un lenguaje moderno, software librefootnote:[Como todos los que usé en los ejemplos de este libro.], implementa el modelo _CSP_ con canales, sus operaciones y la creación de hilos ligeros son construcciones sintácticas. Además permite la ejecución en paralelo especificando el número de hilos nativos del sistema operativo (_threads_) que pueden crearse. Estas características favorecen la especificación compacta y legible de los algoritmos, por ello todos los ejemplos de este capítulo están en Go.


Go incluye dos mecanismos para facilitar la programación concurrente y la ejecución en paralelo en múltiples procesadores:


Hilos ligeros (o _green threads_):: Las llamadas a funciones precedidos por la instrucción +go+ -llamadas _goroutines_- hacen que éstas se ejecuten de forma asincrónica, como un hilo independiente. No son hilos nativos del sistema operativo sino una pequeña pila de tamaño variable gestionada y planificada (_scheduling_) internamente por las librerías _runtime_, su coste de creación es muy bajo. Independientemente de las _goroutines_ también puede crear hilos nativos para ejecución en paralelo. El número de hilos nativos se define con +runtime.GOMAXPROCS+, la planificación y ejecución de las _goroutines_ en los diferentes hilos nativos se hace de forma automática y transparente al programador.


Canales:: Los canales son objetos de primer orden, pueden ser pasados como argumentos en funciones, _goroutines_ y hasta en mensajesfootnote:[Por ello se dice que Go también implementa el modelo _cálculo-π_.]. La implementación de canales está directamente inspirado de _CSP_ y con ideas ya usadas en Newsqueak y Limbo, diseñados también por Rob Pike. Por defecto los canales son sincrónicos como en _CSP_ pero también pueden ser asincrónicos si se especifica el tamaño de _buffer_ al momento de la inicialización (con +make+). Los canales son de tipo estático, pueden ser tipos nativos o cualquier estructura o tipo definido por el programador. La operación de envío o recepción es mensajes tiene la forma +recipiente *<-* origen+, donde +recipiente+ y +origen+ pueden ser indistintamente canales o variables.

La siguiente línea crea un canal de tipo entero:

    ch := make(chan int)

El canal +ch+ tiene _buffer_ cero por lo que será un canal sincrónico, si se desea un canal asincrónico hay que especificar el tamaño del _buffer_ en el segundo argumento de +make+:

    ch := make(chan int, 256)

Enviar el contenido de una variable a un canal:

    ch <- message

Leer un mensaje de un canal y almacenarlo en la variable +message+:

    message = <-ch

Leer un mensaje de un canal y descartar su valor:

    <-ch

En los ejemplos en Go de capítulos anteriores se usó el patrón anterior con el canal +done+ para hacer que el programa principal espere por la finalización de las _goroutines_:

[source, go]
----
func run(done chan bool) {
    ...
    done <- true
}

func main() {
    done := make()
    go run(done)
    <-done
}
----

Dado que implementan variantes del modelo _CSP_ y gestionan los _hilos ligeros_ de forma muy similar, es inevitable -y habitual- la comparación entre Erlang y Go. Aunque ambos implementan el modelo _CSP_ derivan de ramas históricas diferentes, sus diferencias claves son:

- En Erlang como en _CSP_ originalfootnote:[Aunque Hoare planteó la alternativa _atractiva_ (sic) equivalente de nombrar o etiquetar a los canales.] se especifica al proceso receptor. En Go se especifica el canal, cualquier número de estos puede recibir o enviar al mismo canal.

- En Erlang se pueden enviar diferentes tipos de mensajes a cada proceso, estos se depositan en un buzón y son recogidos según las reglas especificadas (_guard commands_) en el receptor. Los canales en Go son de tipos estáticos y la entrega de mensajes es en orden FIFO.

- Erlang sigue el modelo de _actores_, no se permite la compartición de memoria entre los diferentes hilos (_share nothing_ forzado). Aunque en Go se recomienda que toda compartición se haga mediante mensajes, es posible compartir datos vía variables globales (como hemos visto en los ejemplo de capítulos anteriores) o incluso pasando punteros en los mensajes.

El siguiente ejemplo de Erlang define una función anónima que recibe un mensaje y lo imprime. El programa crea un nuevo hilo ligero con +spawn+ y almacena su identificación en +Pid+, posteriormente le envía el mensaje +Hello+ (con el símbolo +!+ como en _CSP_ original de Hoare):

[source, erlang]
----
Pid = spawn(fun() ->
          receive Message ->
            io:format("Message: ~s", [Message])
          end
      end).

Pid ! "Hello".
----

El siguiente es el programa equivalente en Go.

[source, go]
----
channel := make(chan string)
go func() {
    fmt.Println("Message:", <-channel)
}()

channel <- "Hello"
----

Los programas son equivalentes y muy similares, las diferencias fundamentales son la especificación del destinatario del mensaje y que en Erlang no hace falta la creación explícita del canal.

=== Barreras

Las <<sync_barrier, barreras de sincronización>> son un buen ejemplo para introducir el uso de canales como mecanismos de sincronización.

==== Barreras binarias
Una <<sync_barrier, barrera>> para dos procesos es, al igual que con semáforos, un ejemplo sencillo para implementar con mensajes. Dos procesos, _A_ y _B_, deben coordinarse. _A_ no debe pasar de un punto hasta que _B_ haya llegado, y viceversa. La solución con semáforos requería dos, con canales es similar, la primera idea suele ser que cada proceso envíe un mensaje a su canal en cuanto llegue al punto de sincronización y a continuación esperar por la recepción de un mensaje del canal del otro proceso. Por ejemplo:

[source,go]
----
    ch_a = make(chan bool)
    ch_b = make(chan bool)

A                   B

...                 ...
ch_a <- true        cha_b <- true
<-ch_b              <-ch_a
...                 ...
----

El código anterior es erróneo, produce interbloqueo, el _runtime_ de Go interrumpirá el programa completo y avisará del _deadlock_. Es un error habitual cuando no se tiene experiencia con sincronización con canales: no tomar en cuenta que ambos canales son sincrónicos por lo que tanto _A_ como _B_ se bloquean al enviar el mensaje y ninguno de ellos podrá continuar hasta que el otro haya recibido el mensaje (<<railroad_quote>>).

El interbloqueo se produce por una _espera circular_, muy similar a la que analizamos con el interbloqueo de los filósofos (<<deadlocks>>). Se puede evitar haciendo que las operaciones no sigan el mismo orden, uno de los procesos recibe primero el mensaje del otro y luego envíe el propio. Por ejemplo (<<barrier_2p_sync_go, código>>):

[source,go]
----
A                   B

ch_a <- true        <-ch_a
<-ch_b              cha_b <- true
----

Para evitar las soluciones asimétricas hay que recurrir a canales asincrónicos. Por defecto los canales tienen _buffer_ 0, por lo tanto son sincrónicos. Pero se puede especificar el tamaño del _buffer_, en este caso es suficiente con tamaño 1 (<<barrier_2p_async_go, código>>):

[source,go]
----
    ch_a = make(chan bool, 1)
    ch_b = make(chan bool, 1)

A                   B

ch_a <- true        ch_b <- true
<-ch_b              <-ch_a
----

Como ambos canales ahora tienen _buffer_ los procesos no se bloquearán si al enviar no hay ningún proceso ya esperando a recibir. Desde el punto de vista de sincronización la idea es similar al valor o _número de permisos_ de los semáforos. Si un semáforo vale cero bloqueará al primer _wait_ pero si es uno el proceso que haga el primer _wait_ podrá continuar (como se hace con los semáforos usados como _mutex_). En los ejemplos de sincronización de este capítulo -y en aplicaciones reales- es habitual recurrir a canales sincrónicos o asincrónicos con _buffer_ de tamaño uno.

==== Barreras generales

Para este algoritmofootnote:[No sé si alguien lo diseñó o publicó antes, no lo he visto, lo escribí desde cero para este libro.] aprovecharemos las dos capacidades de los mensajes: sincronización y comunicación. En los soluciones con semáforos usamos dos de ellos, uno para contabilizar los procesos que faltan por llegar a la meta y el otro para los que ya habían salido para comenzar la siguiente fase. También usaremos dos canales con el mismo objetivo pero en vez de variables compartidas -sujetas a los problemas de condiciones de carrera- el contador estará almacenado en un mensaje que se irá copiando entre procesos: cada uno lo recogerá, actualizará y volverá a enviar (<<barrier_go, código>>).

Se requieren dos canales de tipo entero, +arrival+ y +departure+, y una variable +n+, esta última es estática, solo se inicializa con el número de procesos que se sincronizarán con la barrera. Definimos la estructura +Barrier+ con estos tres components:


[source,go]
----
type Barrier struct {
    arrival   chan int
    departure chan int
    n         int
}
----

Y una función constructora que inicializará ambos canales y el valor de +n+:

[source,go]
----
func NewBarrier(value int) *Barrier {
    b := new(Barrier)
    b.arrival = make(chan int, 1)
    b.departure = make(chan int, 1)
    b.n = value

    b.arrival <- value  <1>
    return b
}
----
<1> Se deja un mensaje en el canal con el número de procesos que faltan por llegar.

Los dos canales tienen _buffer_ de tamaño uno pero sólo uno de ellos (+arrival+) contiene inicialmente un mensaje con el número de procesos que deben sincronizarse. La función de sincronización +Barrier+ tiene dos partes bien diferenciadas:

1. Llegadas: Se opera sobre el canal +arrival+, inicialmente con un mensaje con el total de procesos que faltan por llegar. Cuando un proceso llega recibe el mensaje, verifica el valor, si quedan procesos por llegar lo decrementa y vuelve a enviar el mensaje al mismo canal. Si es el último en llegar no depositará el mensaje en +arrival+ sino en +departure+ con el total de procesos que se sincronizan en la barrera.

2. Salidas: Los procesos que ya llegaron al final de la fase intentan leer un mensaje de +departure+ y quedarán bloqueados hasta que llegue el último. Cuando éste deposite un mensaje se despertará uno de los bloqueados y verificará el valor, si quedan procesos por salir decrementará su valor y depositará nuevamente el mensaje +departure+ para que puedan continuar los demás. El último en salir enviará un mensaje a +arrival+ para que el ciclo vuelva a comenzar.


[source,go]
----
func (b *Barrier) Barrier() {
    var v int

    // Part 1
    v = <-b.arrival         <1>
    if v > 1 {
        v--
        b.arrival <- v      <2>
    } else {
        b.departure <- b.n  <3>
    }

    // Part 2
    v = <-b.departure       <4>
    if v > 1 {
        v--
        b.departure <- v    <5>
    } else {
        b.arrival <- b.n    <6>
    }
}
----
<1> Se bloquea hasta que puede leer un mensaje desde +arrival+, el mensaje contiene el número de procesos que quedan por llegar.
<2> Si todavía quedan procesos por llegar decrementa el contador y vuelve a poner el mensaje en +arrival+.
<3> Si llegaron todos deposita un mensaje en +departure+ para que los procesos puedan empezar la siguiente fase.
<4> Quedan bloqueados hasta que el último que llegue envíe un mensaje al canal.
<5> Si todavía quedan procesos por salir (bloqueados en +departure+) decrementa el contador y vuelve a poner el mensaje.
<6> Si llegaron todos pone el mensaje con el número inicial de procesos en el canal de llegada.

Como la recepción y envío son operaciones atómicas no hace falta recurrir a ningún método de exclusión mutua. Además, como es un único mensaje los siguientes procesos quedarán bloqueados hasta que el anterior vuelva a depositar un mensaje en el canal lo que asegura  que no se producen condiciones de carrera como ocurre con variables compartidas (hace falta asegurar exclusión mutua explícitamente).

=== Productores-consumidores

Los canales son productores-consumidores por diseño, no hay que hacer nada especial. Los mensajes son los elementos que se añaden o quitan del _buffer_. Si el canal no tiene _buffer_ la comunicación es sincrónica, los productores siempre se bloquean hasta que un consumidor esté preparado para recibir. Si por el contrario se le asigna un _buffer_ funciona exactamente como el modelo de productores-consumidores con _buffer limitado_ que resolvimos con semáforos o monitores.

La interacción es así de sencilla (<<producer_consumer_go, código>>):

[source,go]
----
    buffer := make(chan string, BufferSize)

func consumer() {
    for {
        element := <-buffer
        ...
    }
}

func producer() {
    for {
        element := produce()
        buffer <- element
    }
}
----

Si el buffer del canal está lleno los productores se bloquearán hasta que los productores eliminen mensajes. Si el canal está vacío los consumidores quedarán bloqueados hasta que los productores añadan nuevos elementos. Este tipo de sincronización es muy útil. Mientras en otros lenguajes hay que implementar mecanismos basados en semáforos o monitores en los lenguajes basados en _CSP_ es una forma natural de interacción entre procesos.

[[channels_mutex]]
=== Mutex
La implementación de _mutex_ con mensajesfootnote:[El paquete +sync+ de Go tiene una implementación +Mutex+ que es más eficiente, usa los semáforos implementados a nivel de librería en el +runtime+ (https://golang.org/src/runtime/sema.go), el lenguaje implementa su propio _scheduler_ y usa técnicas de _spin/park_ similares a los usados por los monitores en la máquina virtual de Java.] también es sencilla (<<channel_mutex_go, código>>), inicialmente se crea un canal con capacidad 1 y se deposita un mensaje vacío (no hace falta compartir datos) que representa un _permiso_ para entrar a la sección crítica.

[source,go]
----
    m := make(Mutex, 1)
    m <- Empty{}
----

En la entrada de la sección crítica se lee del canal, como hay un mensaje en el _buffer_ podrá continuar inmediatamente, el siguiente proceso se bloqueará al no tener mensaje que recibir. El proceso que sale de la sección crítica deposita nuevamente un mensaje vacío que permitirá que entre otro o desbloqueará al primer proceso bloqueado.

[source,go]
----
func Lock() {
    <-m
}

func Unlock() {
    m <- Empty{}
}
----


Los canales también bloquean si se intenta enviar un mensaje y el _buffer_ está lleno, por lo que el _mutex_ puede ser implementado a la inversa. Un mensaje representaba a un _permiso_ pero se puede hacer que éste se represente por espacio libre en el _buffer_. En este caso no hace falta depositar un mensaje en la inicialización, en el _lock_ se envía un mensaje y en el _unlock_ se recibe.


[source,go]
----
    m := make(Mutex, 1)

func Lock() {
    m <- Empty{}
}

func Unlock() {
    <-m
}
----

=== Semáforos

Para semáforos generales se puede usar la misma idea que con la primera versión anterior de _mutex_ (<<channel_semaphore_go, código>>), cada mensaje representa un permiso. Solo hace falta una cola a la que hay que iniciar con tantos mensajes como el valor inicial del semáforo:

[source,go]
----
func NewSem(value int) Sem {
    s := make(Sem, 256)
    for i := 0; i < value; i++ {
        s <- Empty{}
    }
    return s
}
----

La operación _wait_ lee un mensaje y _signal_ envía uno vacío:

[source, go]
----
func (s Sem) Wait() {
    <-s
}

func (s Sem) Signal() {
    s <- Empty{}
}
----

El problema de esta solución es la dimensión del _buffer_ del canal, su tamaño debe ser igual al número máximo de permisos del semáforo (el valor máximo de su valor), de lo contrario las operaciones _signal_ también se bloquearán si está lleno. Si no se requieren valores elevados es una solución razonable, si no es así hay que buscar otra solución que no requiera que la dimensión del canal dependa del valor del semáforo.

Una solución de este tipo requeriría, como en los algoritmos de barreras o productores-consumidores, de una cola para mantener un mensaje con el valor actual del semáforo (+value+) y otra cola para bloquear en _wait_ si el semáforo toma un valor negativo (+queue+). La solución no es muy diferente a la simulación de <<monitors_semaphores, semáforos con monitores>> o la implementación del <<futex_semaphore, semáforo con FUTEX>>. En el primer caso usamos la cola de la variable de condición para bloquear a los procesos, en el segundo la cola del FUTEX. Para la siguiente solución usamos el canal +queue+ para mantener la cola de bloqueados.

La estructura e inicialización es la siguiente (<<channel_semaphore2_go, código>>):

[source, go]
----
type Sem struct {
    value chan int
    queue chan Empty
}

func NewSem(value int) Sem {
    var s Sem
    s.value = make(chan int, 1)
    s.queue = make(chan Empty)
    s.value <- value            <1>
    return s
}
----
<1> El canal +value+ se inicializa con un mensaje que almacena el valor del semáforo.

Los algoritmos de las operaciones _wait_ y _signal_ son prácticamente idénticos a la <<semaphore_definition, definición>> de semáforos. La diferencia es que en vez de una variable compartida usamos un mensaje para almacenar el valor.

La función +Wait+ lee el mensaje con el valor del semáforo, lo decrementa y vuelve a depositar el mensaje en el canal. Si el valor del semáforo es menor que cero se bloqueará en el canal +queue+ hasta que otro proceso haga un _signal_.

[source, go]
----
func (s Sem) Wait() {
    v := <-s.value
    v--
    s.value <- v
    if v < 0 {
        <-s.queue
    }
}
----

+Signal+ es la inversa, incrementa el valor del semáforo, si el resultado es menor o igual que cero hay procesos esperando un mensaje en el canal +queue+ por lo que enviará un mensaje para que se despierte el siguiente.

[source, go]
----
func (s Sem) Signal() {
    v := <-s.value
    v++
    s.value <- v
    if v <= 0 {
        s.queue <- Empty{}
    }
}
----

Puede parecer que aparecerán _condiciones de carrera_ porque el envío y recepción en +queue+ se hacen luego de enviar el mensaje pero no existe ese problema. Si al llamar al _wait_ la variable local +v+ es menor que cero el proceso obligatoriamente debe esperar un mensaje (en +queue+). El de _signal_ espera que se haga así y enviará siempre el mensaje correspondiente.

Pero el algoritmo puede optimizarse con una breve modificación en el canal +queue+. Si un proceso en _wait_ ejecuta `s.value <- v` y se interrumpe, el proceso que hace el _signal_ se bloqueará momentáneamente en `s.queue <- Empty{}`, como canal es sincrónico no podrá continuar hasta que el primero ejecute `<-s.queue`. Se soluciona haciendo que el canal +queue+ tenga un _buffer_ pequeño, por ejemplo `s.queue = make(chan Empty, 1)`. No cambia el algoritmo, sigue siendo correcto pero la diferencia es notablefootnote:[En el ejemplo de incrementar el contador los tiempos se reducen hasta cuatro veces.].


=== Filósofos cenando
La solución natural con canales asincrónicos es definir un array de tantos canales como tenedores (<<channel_philosophers_go, código>>), durante la inicialización se deposita un mensaje en cada uno de ellos indicando su disponibilidad:

[source, go]
----
var forks [Philosophers]chan Empty

for i := range forks {
    forks[i] = make(chan Empty, 1)
    forks[i] <- Empty{}
}
----

Para tomar los tenedores cada filósofo lee de los canales correspondientes de cada tenedor. Si está disponible habrá un mensaje y podrá continuar, caso contrario se quedará bloqueado hasta que el tenedor sea liberado. Para evitar interbloqueos (ya analizados en la <<dining_philosophers, solución con semáforos>>) evitamos la espera circular haciendo que siempre se tome primero el tenedor con el menor identificador y luego el de mayor:


[source,go]
----
func pick(id int) {
    if id < right(id) {
        <-forks[id]
        <-forks[right(id)]
    } else {
        <-forks[right(id)]
        <-forks[id]
    }
}
----

Para liberar los tenedores es suficiente con enviar un mensaje a sus canales correspondientes. Si otros filósofos están esperando se desbloquearán inmediatamente.

[source, go]
----
func release(id int) {
    forks[id] <- Empty{}
    forks[right(id)] <- Empty{}
}
----

==== Con canales sincrónicos

El algoritmo anterior solo funciona con canales asincrónicos, de no ser así ni siquiera la inicialización funcionaría porque se bloquearía al ejecutar el primer `forks[i] <- Empty{}` . En el modelo _CSP_ los canales son sincrónicos y Hoare propuso una solución correctafootnote:[Aunque produce interbloqueo, lo avisa en el mismo artículo.].

[[philosophers_hoare]]
.Filósofos en _CSP_
image::hoare_philosophers.png[height="180", align="center"]

La solución es más sencilla de lo que parece (<<channel_philosophers_sync_go, código>>), hay que hacer como se propone en el modelo _CSP_, crear un proceso adicional para cada tenedor (+fork+), los filósofos no requieren de ningún cambio en su algoritmo. Cada proceso +fork+ no requiere de ninguna computación adicional, solo recibe y envía mensajes por el canal correspondiente a su tenedor:

.Proceso para el tenedor _i_
[source,go]
----
func fork(i int) {
    for {
        forks[i] <- Empty{}
        <-forks[i]
    }
}
----

[NOTE]
====
Al tratarse de canales sincrónicos se puede invertir el orden de envío y recepción de mensajes, para tomar los tenedores los filósofos envían un mensaje y al soltarlos reciben uno. En este caso el proceso +fork+ debe invertir también sus operaciones:

    for {
        forks[i] <- Empty{}
        <-forks[i]
    }

De esta forma -es equivalente- el programa queda idéntico a la solución propuesta por Hoare con _CSP_.
====

Los procesos comunicados por canales asincrónicos pueden ser convertidos -tal como acabamos de hacer- a uno equivalente con canales sincrónicos. La solución general es añadir nuevos procesos que suplanten las capacidades de los canales con _buffer_. En el caso de los filósofos añadimos un nuevo proceso para cada tenedor para convertirlo en una comunicaciones entre procesos _filósofos_ y otros _tenedores_. Para el <<channel_mutex_go, código>> de simulación de _mutex_, por ejemplo, se requieren muy pocos cambios. La función _pseudo-constructora_ de +Mutex+ con canales asincrónicos crea un canal con _buffer_ 1 y deposita un mensaje:


[source,go]
----
func NewMutex() Mutex {
    m := make(Mutex, 1)
    m <- Empty{}
    return m
}
----

Dado que no podemos hacerlo con canales sincrónicos se requiere otro proceso que actúe de como el proceso +fork+ de los filósofos. Se puede hacer que el propio constructor inicie el nuevo proceso sin necesidad de modificar la implementación de las otras funciones (<<channel_mutex_sync_go, código completo>>)footnote:[Uso función anónima con clausura, de lectura y comprensión más sencilla.]:

[source,go]
----
func NewMutex() Mutex {
    m := make(Mutex)
    go func() {         <1>
        for {
            m <- Empty{}
            <-m
        }
    }()
    return m
}
----
<1> Se lanza una _goroutine_, la función es anónima y aprovecha de la clausura para hacer referencia al mismo canal +m+.

==== Solución óptima
La solución anterior (ya la analizamos <<dining_philosophers_semaphores, con semáforos>>) no asegura que puedan comer todos los filósofos que podrían hacerlo. Se puede implementar una solución óptima similar a la de semáforos pero adaptada a canales (<<channel_philosophers_provider_go, código completo>>).

En vez de solicitar los tenedores individualmente habrá un proceso _proveedor_ (+provider+) para toda la mesa, este proceso usará un único canal sincrónico para recibir los mensajes de todos los filósofos. Estos enviarán mensajes indicando si quieren tomar o soltar los tenedores. El proveedor verificará el estado de los filósofos vecinos, si ambos tenedores están libres le responderá con un mensaje para que continúe. Si alguno de sus vecinos está comiendo no le responderá inmediatamente sino cuando sus vecinos hayan dejado de comer.

El mensaje de filósofos al proveedor será una estructura que indicará el índice el filósofo, el estado (+Hungry+ si desea comer y +Thinking+ si es para liberar los tenedores) y el canal individual del filósofo (también sincrónico) para recibir la respuesta del proveedorfootnote:[Go permite enviar descriptores de canales en los mensajes por lo que no hace falta que estos sean parte del estado global, cada filósofo crea el suyo y lo pasa el proveedor en el mensaje.]:

[source, go]
----
type Request struct {
    id     int
    status int
    c      chan Empty
}
----

Cuando un filósofo desea comer envía un mensaje al canal del proveedor con su identificación (+i+), su canal (+myCh+) y el estado +Hungry+. A continuación espera la respuesta del proveedor:

[source, go]
----
provider <- Request{id: i, c: myCh, status: Hungry}

<-myCh
----

Cuando libera los tenedores envía otro mensaje similar pero con el estado +Thinking+:

[source, go]
----
provider <- Request{id: i, c: myCh, status: Thinking}
----

El proveedor mantiene un array que con el estado de los filósofos y su canal de comunicación. Inicialmente cada posición es una copia de la estructura +Request+ de los mensajes. El proceso está en un bucle infinito recibiendo mensajes desde su canal +provider+, cuando recibe uno lo copia al array de estado y verifica el estado:

1. Si es +Hungry+ llama a la función +canEat+, esta función responderá con un mensaje al canal del filósofo si puede comer.

2. Si el estado es +Thinking+ significa que deja los tenedores por lo que se llama a la función +canEat+, una vez para cada vecino por si querían comer y están esperando.

[source, go]
----
for {
    m := <-provider
    philo[m.id] = m
    switch m.status {
    case Hungry:
        canEat(m.id)
    case Thinking:
        canEat(left(m.id))
        canEat(right(m.id))
    }
}
----

La función +canEat+ es idéntica a la homónima de la solución óptima con semáforosfootnote:[Nuevamente aparecen las similitudes de sincronización entre semáforos y canales.] (<<philosophers_2_py, código Python>>), solo que en vez de señalizar un semáforo se responde con un mensaje. La función verifica el estado de los vecinos a izquierda y derecha del filósofo indicado en el argumento (+i+), si ninguno de los vecinos está comiendo entonces permite continuar enviando un mensaje al canal correspondiente.

[source, go]
----
func canEat(i int) {
    r := right(i)
    l := left(i)
    if philo[i].status == Hungry &&
        philo[l].status != Eating &&
        philo[r].status != Eating {
        philo[i].status = Eating
        philo[i].c <- Empty{}
    }
}
----

=== Paralelismo
En 1979, poco después de la publicación del artículo del modelo _CSP_ la empresa británica INMOfootnote:[Actualmente STMicroelectronics, http://www.st.com/.] pidió colaboración a Hoare para crear el lenguaje occam para su nueva arquitectura de multiprocesamiento masivo _Transputer_. A principios de la década de 1980 se pensaba que se había llegado al límite de la capacidad de los procesadoresfootnote:[Podían poner más transistores en un chip pero no sabían qué hacer con ellos, luego surgieron las arquitecturas _superescalares_ que permitieron aumentar la potencia de cálculo, lo que también significó la decadencia de _Transputer_.] por lo que diseñaron una arquitectura basada en el modelo _CSP_. Consistía de procesadores con instrucciones genéricas, 4 KB de RAM incluidas en el chip y cuatro puertos series de alta velocidad. Cada puerto podía usarse para conectar a otros procesadores y así formar arrays de procesasores con canales sincrónicosfootnote:[Llegaron a fabricar un _switch_ de red de 32x32 procesadores.].


[[BOO42]]
.Placa con Transputer con matriz de 6x7 procesadoresfootnote:[De la página David May, uno de los arquitectos de Transputer, https://www.cs.bris.ac.uk/~dave/transputer.html]
image::B0042.jpg[width="300", align="center"]

Inicialmente solo se podía programar en occam pero luego se adaptaron librerías para lenguajes como Pascal, C y Fortran, también se desarrollaron y portaron varios sistemas operativos como Minix, Paros y Trollius. Aunque inicialmente tuvo éxito en el ambiente académico (ofrecía buena potencia de cálculo, sobre todo de matrices) y se usó en sistemas satelitales desapareció posiblemente por la aparición de microprocesadores más potentes y económicos, o por el desconocimiento general de concurrencia de los programadores de la época. Aunque ya no existe su arquitectura influyó notablemente en el desarrollo de los chips para tratamiento digital de señales, la supercomputación basada en _clusters_ y hasta la conocida _Blue Gene_ de IBM que soporta miles de procesadores conectados por canales de alta velocidadfootnote:[Está basada en la arquitectura QCDOC, originalmente soportaba canales de comunicacion con 12 nodos vecinos y hasta 12 Gbits/seg.]


==== Multiplicación de matrices en paralelo

Una muestra de la potencia del modelo _CSP_ en arquitecturas con múltiples procesadores es el producto de matrices. Aunque el siguiente ejemplo trata con matrices y enteros pequeños su uso estaba orientado a matrices de grandes dimensiones que compensen la sobrecarga y demoras provocados por el envío de mensajes. Analizaremos el algoritmo para multiplicar en paralelo dos matrices de 3x3, como las de la siguiente imagen:

[[matrix_multiplication]]
image::matrix_multiplication.png[width="360", align="center"]

Cada elemento de la matriz resultante puede ser calculado independientemente, por ejemplo el elemento central de la matriz (25) se calcula de la siguiente forma:

[[element_multiplication]]
image::element_multiplication.png[width="400", align="center"]

El cálculo se puede descomponer en diferentes procesos _multiplicadores_ comunicados por canales. Cada uno de ellos multiplican un elemento de cada matriz, lo añaden a la suma parcial recibida desde otro proceso y envían el resultado al siguiente multiplicador. Para matrices de 3x3 se necesitan tres procesos por fila inicializados con los valores de una fila de la primera matriz ([4, 5, 6]). Del canal _norte_ (_north_)footnote:[Recordad que cada procesador de _Transputer_ tiene cuatro puertos, para ubicarlos en el diagrama los llamamos _norte_, _este_, _sur_ y _oeste_.] reciben un elemento de la fila correspondiente a la segunda matriz ([2, 1, 2]):

[[col_row_multiplication]]
image::col_row_multiplication.png[width="480", align="center"]

Para obtener el resultado final en el procesador de la izquierda cada proceso multiplica el valor inicial por el que le llegó desde el _norte_, lo suma al resultado desde el canal del _este_ y lo envía en su canal del _oeste_. El proceso _zero_ de la columna de la derecha únicamente envía ceros para iniciar la suma parcial para que los algoritmos de cada multiplicador sean idénticos:

[source, go]
----
second := <-north
sum := <-east
west <- sum + first*second
----

Tal como ya había descrito Hoare se puede generalizar para la multiplicación en paralelo de la matriz completa con nueve _multiplicadores_ (en el centro de la imagen). Los procesos de la fila superior envían los valores, uno a uno, de las filas de la segunda matriz, los resultados parciales lo obtienen los procesos de la columna izquierda. (_result_). Cada multiplicador copia el mensaje recibido del canal _norte_ al canal _sur_ para que procesos de las siguientes filas (se añaden los procesos _sink_ de la fila inferior con el único objetivo de que el algoritmo sea el mismo para todos los multiplicadores).


[[parallel_multiplication]]
.Array de procesos para multiplicación de matrices
image::parallel_multiplication.png[width="480", align="center"]

El algoritmo de cada uno de los cuatro tipos de procesos de la _matriz de procesos_ es el siguiente (<<parallel_matrix_multiplication_go, código completo>>):

[source, go]
----
func multiplier(first int) {
    for {
        second := <-north
        south <- second
        sum := <-east
        west <- sum + first*second
    }
}

func result(rowNum int) {
    for i := 0; i < Dim; i++ {
        row[i] := <-east
    }
}

func source(row Row) {
    for i := range row {
        south <- row[i]
    }
}

func zero(west chan int) {
    for {
        west <- 0
    }
}

func sink() {
    for {
        <-north
    }
}
----

=== Algoritmos distribuidos

No es el objetivo de este libro, se necesitaría uno específico y bastante extenso dado el avance y cantidad de sistemas y protocolos que se desarrollaron en los últimos años. Pero no podía dejar de mencionarlo, los canales de comunicación y el modelo de _procesos comunicados_ son elementos fundamentales de los sistemas distribuidos. A estos se le añade  un tercer elemento: los _nodos_, ordenadores independientes conectados solo por un canal de comunicaciónfootnote:[De diferentes características, fundamentalmente si son fiables y entregan los mensajes en el mismo orden en que lo reciben.] (_débilmente acoplados_) y pueden ejecutar más de un proceso.


En sistemas distribuidos hay que tener en cuenta otros requerimientos y problemas que no existen en procesos concurrentes en memoria compartida:

- Los canales y nodos pueden fallar sin notificar a los demás procesos por lo que hay que considerar tiempos y caducidad.

- El grafo o estructura de la red de nodos puede ser variable, compleja y no permitir la conexión de cada nodo con todos los demás.

- No se pueden tomar decisiones suponiendo un número fijo de nodos o procesos y que cada uno de ellos recibió cada mensaje, se requieren pasos adicionales de sincronización y verificación.

- La operación que más tiempo toma es la copia de mensajes de un nodo a otro por lo que la prioridad es reducir el tamaño y número de mensajes.

==== Estructura de procesos distribuidos
Los procesos distribuidos deben responder a mensajes de sincronización que llegan desde otros nodos, lo habitual es implementar al menos un hilo auxiliar independiente responsable de recibir los mensajes de la red y responder adecuadamente lo antes posible. Un proceso que se ejecuta en un nodo (_Process_) consiste de un hilo principal (_Main_) y un auxiliar (_Receiver_) que comparten memoria y se sincronizan entre ellos con cualquiera de los mecanismos de memoria compartida vistos en este libro.

[[distributed_process]]
image::distributed_process.png[width="240", align="center"]

Los programas diseñados según los principios de _CSP_ y cuyo único mecanismo de comunicación y compartición de datos son mensajes pueden ser fácilmente portados a sistemas distribuidos cambiando las primitivas _send_ y _receive_ de canales locales por sistemas de gestión de _colas de mensajes_ (como Beanstalkd o RabbitMQ). Por el mismo principio, algoritmos diseñados para ser distribuidos pueden ser fácilmente implementados y simulados localmente con el modelo _CSP_.

===== Simulación en Go
Los siguientes ejemplos de exclusión mutua simulan un sistema distribuido respetan la estructura de la imagen. Cada nodo es una _goroutine_ que a su vez pone en marcha otra _goroutine_ y con la que comparte memoria. El patrón de los programas es el siguiente:

----
func node(aChannel chan Struct) {
    number := 0
    mutex := new(sync.Mutex)

    receiver := func() {
        for {
            request := <-aChannel
            // ...
            aChannel <- response()
            }
        }
    }
    go receiver()
    mainProcessing()
}

func main() {
    //...
    go node(aChannel)
}
----

Desde el programa principal se pone en marcha un _nodo_ llamando a la función +node+, el núcleo del proceso principal que hace el _trabajo real_. En ella se definen las variables compartidas necesarias y pone en marcha el hilo de sincronización +receiver+. En Go es una clausura, las variables definidas en +node+ son accesibles desde la función anónima de +receiver+.


==== Exclusión mutua distribuida
Como breve introducción al diseño de algoritmos distribuidos analizaremos uno de los algoritmos más conocidos, el de exclusión mutua distribuida por _autorización_ de Ricart-Agrawala (<<Ricart>>, 1981) basado en el conocido algoritmo de la panadería.

Al tratarse de exclusión mutua usamos las funciones +Lock+ y +Unlock+, que son las que deben sincronizarse para asegurar la exclusión mutua entre los diferentes nodos. Como en todos los ejemplos previos, el programa incrementa la variable compartida +counter+,
no tiene sentido ni es posible en un sistema distribuido real pero nos sirve para verificar el funcionamiento. Tampoco es usual que se requieran secciones críticas globales en un sistema distribuido, pero la relativa simplicidad del modelo y los algoritmos son muy útiles para una rápida introducción a la _sensación_ de diseñar algoritmos distribuidosfootnote:[Por otro lado, un área apasionante.].

===== Algoritmo de Ricart-Agrawala (1981)

Es uno de los algoritmos distribuidos más sencillos de interpretar, el proceso que desea entrar a la sección crítica debe recibir la autorización de todos los demás (<<distributed_me1_go, código fuente>>). Para ello envía un mensaje a los demás y espera la respuesta de todos (cada entrada requiere +2(n-1)+ mensajes). Estos solo responderán si no hay competencia y no están en la sección crítica o el número del remitente es menor. Como en el algoritmo de la panadería, el turno de entrada se asigna por un número creciente.

Cada proceso almacena el máximo número que recibió desde la red (+highestNum+), el número que selecciona cuando desea entrar a la sección crítica (+myNumber+), una cola de las respuestas pendientes a otros procesos que desean entrar (+deferred+), una variable booleana para indicar que el proceso está esperando para entrar a la sección crítica (+requestCS+), y un _mutex_ para la sincronización entre el proceso principal y +receiver+.

.Variables
[source, go]
----
highestNum := 0
myNumber := 0
deferred := make(chan int, Nodes)
requestCS := false
mutex := new(sync.Mutex)
----

En +Lock+ se indica que se quiere entrar a la sección crítica (lo necesita el hilo +receiver+), se selecciona el número igual al más alto visto más uno y se envía un mensaje a todos los demás procesos con el identificador y número seleccionado. Luego se espera a recibir la respuesta de todos, cuando lleguen todas podrá continuar.

.Lock
[source, go]
----
mutex.Lock()            <1>
requestCS = true
myNumber = highestNum + 1
mutex.Unlock()          <1>

for i := range requests {
    if i == id {
        continue
    }
    requests[i] <- Message{source: id, number: myNumber}
}

for i := 0; i < Nodes-1; i++ {
    <-replies[id]
}
----
<1> Hay que asegurar exclusión mutua para evitar condiciones de carreras con el hilo de +receiver+.

La tarea fundamental de +Unlock+ es enviar una respuesta a todos los procesos que enviaron solicitudes (las recibió el hilo +receiver+) mientras se estaba en la sección crítica.

.Unlock
[source, go]
----
requestCS = false
mutex.Lock()            <1>
n := len(deferred)
mutex.Unlock()          <1>
for i := 0; i < n; i++ {
    src := <-deferred   <2>
    replies[src] <- Message{source: id}
}
----
<1> Hay que asegurar exclusión mutua para evitar condiciones de carreras con +receiver+.
<2> Envía la respuesta a los que están pendientes de respuesta. Fueron añadidos a +deferred+ por +receiver+.

El hilo +receiver+ se ejecuta asincrónicamente esperando peticiones de los otros procesos, sus mensajes incluyen el identificador del proceso y el número que seleccionaron (con en la panadería, puede haber números repetidos). Cuando recibe una petición responde inmediatamente si el proceso local no desea entrar a la sección crítica o el número del proceso remoto es menor. En caso contrario agrega el identificador del proceso remoto a la cola +deferred+ para que se le envíe la respuesta desde +Unlock+.

._Receiver_
[source, go]
----
for {
    m := <-requests[id]
    mutex.Lock()
    if m.number > highestNum {  <1>
        highestNum = m.number
    }
    if !requestCS ||
        (m.number < myNumber || <2>
        (m.number == myNumber &&
            m.source < id)) {   <3>
        mutex.Unlock()
        replies[m.source] <- Message{source: id}
    } else {
        deferred <- m.source    <4>
        mutex.Unlock()
    }
}
----
<1> Actualiza +highestNum+ si el número recibido de otro proceso es mayor.
<2> La comparación es similar a la del <<bakery, algoritmo de la panadería>>.
<3> Si el proceso no desea entrar a la sección crítica o el número del otro proceso es menor envía la respuesta inmediatamente.
<4> Si no, agrega el proceso a los _retrasados_ para que se envíe la respuesta después de salir de la sección crítica.

===== Algoritmos basados en paso de testigo

El algoritmo anterior no es el único ni el más eficiente, se desarrollaron otros que minimizan la cantidad de mensajes. Dos de los más estudiados son el de paso de testigos (_token-passing_) de Ricart-Agrawala (<<Agrawala>>, <<Carvalho>>) y el Neilsen-Mizuno (<<Neilsen>>). Los algoritmos de paso de testigo requieren +n+ mensajes cada vez que se solicita el testigo, es una reducción importante y si el proceso que desea entrar ya tiene el testigo no hace falta que vuelva a solicitarlo. No solo decrementa el número de mensajes, también reduce notablemente las demoras en la entrada.


====== _Token-passing_ de Ricart-Agrawala (1983)
Reduce considerablemente el número de mensajes (<<distributed_me2_go, código fuente>>). Para acceder a la sección crítica el proceso debe poseer el testigo (_token_), solo uno de ellos puede tenerlo. Si el proceso que desea entrar a la sección crítica no lo posee debe solicitarlo enviando una solicitud a todos los demás, el que tenga el testigo se lo pasará cuando salga de su sección crítica.

La elección de a quién le corresponde el testigo también se hace por el número elegido por cada proceso pero a diferencia del anterior no se usa un número único, cada proceso mantiene un par de arrays con los números de todos los demás. El primero (+requested+) es el número con el que solicitó el testigo cada proceso. El segundo (+granted+) el número con que se le otorgó el testigo por última vez a cada proceso. Para elegir al siguiente se  se selecciona uno cuyo número de solicitud (en +requested+) sea mayor al número de la última vez que se le otorgó el testigo (en +granted+).

Cuando se pasa el testigo de un proceso a otro también se envía el array +granted+, así se asegura que el que toma la decisión tiene la versión actualizada. El tamaño de ambos arrays es proporcional al número de nodos, es un problema para grandes redes por el espacio de almacenamiento en cada nodo como por el tamaño del mensaje cuando se transfiere el testigo.


====== _Token-passing_ de Neilsen-Mizuno (1991)
Elimina el problema de almacenar y transferir el array (<<distributed_me3_go, código fuente>>). Cada nodo solo mantiene dos variables enteras, el _padre_ (+parent+) del proceso y el identificador del siguiente nodo al que le corresponde el testigo (+deferred+).

El algoritmo se basa en la creación de árboles virtuales, +parent+ indica cuál es el padre de un proceso (así se define un árbol virtual). Inicialmente hay que asignar un padre a cada nodo para definir un árbol de cobertura (_spanning tree_) virtual, en el código de ejemplo todos se hacen hijos del proceso 0.

Cuando un proceso solicita el testigo envía un mensaje a su padre e inmediatamente se _desconecta_ del árbol (formará otro nuevo) poniendo su +parent+ en -1. Si el receptor del mensaje no tiene el testigo envía una copia del mensaje al padre y selecciona al remitente anterior como su nuevo padre.

Supongamos que _A_ solicita el testigo y que lo tiene _D_. La situación inicial es:

+_A_ => _B_ => _C_ => *_D_* <= _E_+

Cuando _B_ recibe el mensaje desde _A_ lo reenvía a _C_ y cambia su padre a _A_:

+_A_ <= _B_   _C_ => *_D_* <= _E_+

El mensaje es así copiado hasta que llega a la raíz del árbol actual ligado al poseedor del testigo (_D_), las conexiones en ese momento serán las siguientes (hay dos árboles, la raíz de uno es el poseedor el testigo, el otro es el siguiente):

+_A_ <= _B_ <= _C_   *_D_* <= _E_+


El proceso puede estar en dos estados:

1. Si no está en la sección crítica transfiere el testigo inmediatamente al proceso original que lo solicitó.

2. Si está en la sección crítica pone al remitente del mensaje original en su +deferred+, será al que pase el testigo cuando haya salido de la sección crítica.

En cualquier de los dos casos, el árbol se habrá unificado.

+*_A_* <= _B_ <= _C_ <= _D_ <= _E_+

====== Eficiencia

Como curiosidad, los tiempos de ejecución (de reloj) en el mismo ordenador del contador <<go_mutex_go, usando _mutex_>> y los algoritmos distribuidos que acabamos de verfootnote:[Recordad que estamos simulando la _distribución_, todos los procesos y canales son locales.].

[[distributed_comparison]]
.Tiempos de ejecución de los algoritmos de EM distribuida
image::distributed_comparison.png[align="center"]


La sobrecarga por la número de mensajes que se envían en el algoritmo de _autorización_ de Ricart-Agrawala es enorme. Los algoritmos de _token passing_ se comportan muy bien considerando que se crean el doble de hilos (ligeros), su lógica es más compleja y que toda la información se copia por mensajesfootnote:[También indica que la implementación de mensajes y el _scheduling_ del _runtime_ de Go es muy eficiente.]

=== Recapitulación

La popularización de ordenadores con número masivo de procesadores, servicios en la _nube_, _microservicios_, plataformas para programación distribuida y tolerante fallos, y hasta nuevos lenguajes de programación que lo incluyen como construcción sintáctica hace que el modelo de sincronización y comunicación con canales esté de moda. Pero pocos desarrolladores conocen sus orígenes (el modelo _CSP_) y los mecanismos básicos de sincronización sobre el que se construyen algoritmos y técnicas más complejas. El objetivo de este capítulo fue llenar este hueco de conocimiento y poner en contexto la historia y equivalencia de canales con los demás mecanismos de sincronización de procesos.

Hemos visto como con canales se pueden resolver los mismos problemas de concurrencia que resolvimos con semáforos y monitores. En general los tres son equivalente como mecanismos de sincronización en sistemas de memoria compartido, si se tiene uno se pueden implementar los otros (con mayor o menor dificultad). A diferencia de los semáforos y monitores los canales tienen la capacidad adicional de servir para la comunicación entre procesos, también pueden ser usados para procesos independientes sin memoria compartida. Esto implica que también son útiles para procesos distribuidos en diferentes nodos, la brevísima introducción a algoritmos distribuidos sirvió como muestra esta capacidad.




////
http://www.slideshare.net/dabeaz/an-introduction-to-python-concurrency (para ver lo de mensajes)
////
