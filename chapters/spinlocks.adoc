[[spinlocks]]
== _Spinlocks_ eficientes
Todas las soluciones de exclusión mutua vistas tienen algo en común, un bucle que continuamentefootnote:[Es decir, en espera activa.] verifica el estado de una variable o registro _RMW_ hasta que el proceso puede entrar a la sección crítica. Estos algoritmos se denominan _spinlocks_, el de Dekker, Peterson, la Panadería o cualquiera de las soluciones de exclusión mutua del capítulo anterior son también _spinlocks_. Su uso solo tiene sentido si se cumple una de las siguientes condiciones:

. El código que se ejecuta en la sección crítica es muy breve y por lo tanto la competencia (_contention_) es baja. Esta es una solución empleada desde los inicios de la informática para sincronizar el núcleo de los sistemas operativos e implementar mecanismos de sincronización que evitan las esperas activasfootnote:[Los que veremos en los capítules siguientes.].

. Los procesos se ejecutan en paralelo en diferentes procesadores, mientras unos procesos están en sus _spinlocks_ otros pueden avanzar y salir de la sección crítica. Estas técnicas permiten evitar mayores pérdidas de tiempo ocasionadas por llamadas de sistema y/o cambio de estado de procesos. Si hay muchos más procesos en _spinlocks_ que procesadores se llega a una situación similar a la de un único procesador por lo que los procesos avanzarían muy lentamente y tendría más sentido utilizar construcciones sin esperas activas.

El problema principal es la ineficiencia provocada por la espera activa y los requerimientos de almacenamiento en las soluciones algorítmicas sin registros _RMW_. Por ello se desarrollaron las primitivas de hardware que permiten minimizar el impacto y sobre todo eliminar la necesidad de recorrer una multitud de registros como en el algoritmo de la Panadería. Estas primitivas suponen una gran mejora en el espacio necesario -requieren sólo una palabra por cada seccción crítica (o _lock_)- como de facilidades de programación. Pero no todas los procesadores ofrecen las mismas, si se quiere obtener el máximo de eficiencia hay que programar para cada arquitectura como se suele hacer en el núcleo de los sistemas operativos. Para facilitar la portabilidad los compiladores incluyen primitivas fundamentales, los macros o _intrinsics_, que son traducidas a las operaciones que implementan o simulan esas primitivas y que son las que usé para el código de demostraciónfootnote:[Salvo el código en ensamblador con ldrex/strex para ARM.].

Analizaremos el comportamiento de cada uno de estos _spinlocks_ y las técnicas más usadas para optimizarlos. En la siguiente figura se muestran los tiempos de _reloj_ en segundos de la ejecución de los algoritmos del capítulo anterior ejecutados sobre un Intel i5 con 4 procesadores (barra azul de la izquierda), ARM v7 de Raspberry Pi 2 con cuatro núcleos (barra naranja), ARM v6 de Raspberry 1 (barra amarilla) y finalmente la de una instancia m3.largefootnote:[Dos núcleos virtuales.] de Amazon EC2.

[[hardware_times]]
.Tiempos de ejecución para los diferentes macros e instrucciones de hardware para Intel y ARM v6
[caption=""]
image::times-hardware.png[align="center"]

Cada grupo es una medición diferente. El primero (_none_) es el tiempo sin ningún mecanismo de exclusión mutua. El segundo (_ultimate_) es el más eficiente para este caso de sólo sumar un entero usando la instrucción atómica _getAndAdd_ directamente sobre la variable `counter`. Los siguientes hacia la derecha son los algoritmos usando las instrucciones de hardware del capítulo anterior _swap_, _getAndAdd_, _testAndSet_, _getAndSet_ y _compareAndSwap_ respectivamente.

Se puede obervar que todos los algoritmos de exclusión mutua imponen un sobrecoste importante pero que no es uniforme para todas las plataformasfootnote:[En _get&add_ no están los tiempos de la Raspberry 1 y la instancia m3.large porque necesitan mucho tiempo, hasta horas.].

En Raspberry 1 la implementación más eficiente es _getAndSet_ seguida por _compareAndSwap_.
En Raspberry 2 la más eficiente es _compareAndSwap_ (hay que recordar que además de arquitectura más moderna son cuatro procesadores). En Intel la más eficiente es _swap_ y la peor es _compareAndSwap_, un resultado curioso dado que Intel tiene _CAS_ nativo pero en ARM se emula con _LL/SC_.footnote:[También muestra las buenas propiedades de LL/SC.]. Es interesante lo bien que se comporta la instancia EC2, y que _CAS_ sea el más eficiente, seguramente influenciado por las características más modernas del procesador.

=== Optimizaciones
Los _spinlock_ son ineficientes por dos razones:

Uso y competencia por el procesador:: Los procesos consumen el 100% CPU verificando el valor de una variable, si hay más hilos compitiendo que procesadores la mayor parte del tiempo se pierde en el bucle de verificación.

La presión sobre la memoria cache:: Estos algoritmos no son _escalables_ En sistemas con varios procesadores todos los procesos verifican el estado de la misma variable (_hot spot_) por lo que se generan mensajes de sincronización de cache entre los diferentes procesadores.

Aunque son preferibles los _spinlocks_ escalables veremos técnicas básicas para mejorar el rendimiento de los _spinlocks_ anteriores, al final del capítulo veremos los dos algoritmos escalables más importantes.

[NOTE]
._Spinlocks_ escalables
====
Se denominan _spinlocks escalables_ aquellos en que los _fallosfootnote:[No implica que haya producido un error en el sistema sino que el procesador no tiene una copia actualizada en su memoria cache por lo que se deben producir intercambios de mensajes para actualizarla al último valor.] de memoria cache_ se mantienen constantes independientemente de las iteraciones necesarias en la entrada a la sección crítica (<<MCS>>, <<Boyd-Wickizer>>).

Si en cada iteración (o _spin_) los procesos en diferentes procesadores verifican las mismas variables se produce -como mínimo- un _fallo de cache_ cada vez que un proceso cambia de estado ya que cada actualización genera invalidaciones de cache. El problema se agrava por el <<false_sharing, _false sharing_>>, los registros en la sección crítica suelen estar en áreas cercanas a las variables de los _spinlocks_. Por ello se estudiaron _spinlocks escalables_, el fundamental es el <<mcs_queue>> que vemos más adelante.

====

Las tres que veremos a continuación, implementados sobre el _spinlock_ con _testAndSet_, no pueden ser considerados _escalables_ ya que no reducen la presión sobre la memoria cache.

==== Verificación local
Se trata de reducir la presión sobre el sistema de coherencia de cache y evitar la llamada a la instrucción atómica verificando antes (_corto circuitando_) el valor de la variable sobre la que se itera -la variable `mutex` en los ejemplos-. Si ésta vale `1` ya no se llama a la instrucción atómica. Esta técnica se la conoce como _TAS_ o _TATAS_ cuando se usa con la instrucción _TAS_.

[source]
----
        mutex = 0

def lock():
    while mutex or TAS(mutex):
        pass
----

La mejora de esta solución depende mucho de la arquitectura y de sus mecanismos de coherencia de cache. En los <<execution_times, tiempos de ejecución>> se observa que esta técnica mejora mucho la eficiencia en las arquitecturas Intel pero son casi despreciables en la arquitectura ARM.

Código fuente para <<test_test_and_set_c, _TAS_>>, <<test_swap_c, _swap_>> y <<test_compare_and_swap_c, _CAS_>>.

==== Ceder el procesador
La idea es sencilla, si un proceso está forzado a continuar en la espera activa porque no se cumple la condición de salida del bucle es mejor ceder el procesador para que otro lo intente o pueda salir antes de la sección crítica. En el ejemplo usamos la llamada de sistema estándar POSIX `sched_yield` que hace que el sistema operativo quite al proceso de ejecución y lo mueva a la cola de _listos para ejecutar_.
[source]
----
        mutex = 0

def lock():
    while mutex or TAS(mutex):
        sched_yield()
----
La cesión del procesador <<execution_times, produce reducciones de tiempo importantes>> en todas las arquitecturas salvo excepciones como en _CAS_ sobre la Raspeberry con un único procesadorfootnote:[La causa pueden ser el coste adicional de llamadas de sistemas y cambios de contexto, o el efecto ping-pong de procesos que cambian de estado continuamente.].

Código fuente para <<test_and_set_yield_c, _TAS_>>, <<swap_yield_c, _swap_>> y <<compare_and_swap_yield_c, _CAS_>>.


==== Espera exponencial
La forma de reducir la competencia (_contention_) y evitar que el efecto ping-pong de los procesos es forzar a que permanezcan bloqueados por un tiempo variable dependiendo de las veces que ha _fallado_.


[NOTE]
.Exponential backoff
====
_Exponential backoff_ es la técnica usada por redes Ethernet, WiFi y similares de _bus compartido_ para calcular el tiempo de espera para reenviar después de una colisión, _backoff_ se refiere a la espera y _exponential_ a que el valor límite del tiempo a esperar se duplica en cada _fallo_. El tiempo efectivo de espera de cada proceso es un número aleatorio entre 1 y el límitefootnote:[Se usa un número aleatorio para evitar que todos los procesos reintenten simultáneamente.].

El siguiente es el código en C usado en los ejemplos. En cada iteración fallida dentro del _spinlock_ el proceso incrementa el contador de fallos (`failures`) y llama a la función _backoff_. Ésta calcula el límite (`limit`) con desplazamiento de bits, cada posición desplazada multiplica por dos, por ello se desplaza el bit `1` hacia la izquierda con un máximo de 12, unos 4096 nanosegundos. Luego se calcula el tiempo que esperará con un número random entre 1 y el límite.


[source,c]
----
#define FAILURES_LIMIT 12
void backoff(int failures) {
    struct timespec deadline = {.tv_sec = 0};
    unsigned limit;

    if (failures > FAILURES_LIMIT) {
        limit = 1 << FAILURES_LIMIT;
    } else {
        limit = 1 << failures;
    }

    deadline.tv_nsec = 1 + rand() % limit;
    clock_nanosleep(CLOCK_REALTIME, 0, &deadline, NULL);
}
----
====


[source, c]
----
        mutex = 0

def lock():
    failures = 0

    while mutex or TAS(mutex):
        failures += 1
        backoff(failures)
----

La dificultad del _backoff_ reside en la elección de la unidad de tiempo de espera, no existe un valor ideal y depende de cada arquitectura y caso de uso. Si la espera es muy breve producirá un efecto similar al `sched_yield` con una sobrecarga aún mayor del sistema operativofootnote:[El proceso pasa de ejecución a _bloqueado_ y de allí a _listo_ y nuevamente a ejecución en un tiempo muy breve.]. Por el contrario, si la unidad es muy grande producirá demoras innecesarias y con las CPUs inactivas ya que todos los procesos involucrados están _bloqueados_.

Código fuente para <<test_and_set_backoff_c, _TAS_>>, <<swap_backoff_c, _swap_>> y <<compare_and_swap_backoff_c, _CAS_>>.

[[execution_times]]
==== Tiempos de ejecución



.Intel i5 cuatro núcleos
image::optimized-intel.png[align="center"]

.Intel AWS m3.large
image::optimized-m3-large.png[align="center"]

.ARMv7 Raspberry 2
image::optimized-arm7.png[align="center"]

.ARMv6 Raspberry 1
image::optimized-arm.png[align="center"]




[[mcs_queue]]
==== MCS Spinlock


.Cola MCS
image::mcs.png[align="center"]

==== CLH Spinlock

[[clh_queue]]
.Cola CLH
image::clh.png[align="center"]


===== Ticket vs MCS vs CLH
image::ticket-mcs-clh.png[align="center"]

===== Ticket vs MCS vs CLH con sched_yield
image::ticket-mcs-clh-yield.png[align="center"]


Agradecimientos a Marc Pampols



Reader-writer: https://jfdube.wordpress.com/2014/01/03/implementing-a-recursive-read-write-spinlock/
https://jfdube.wordpress.com/2014/01/12/optimizing-the-recursive-read-write-spinlock/



(http://nullprogram.com/blog/2014/09/02/ https://github.com/skeeto/lstack)
Common Pitfalls in Writing Lock-Free Algorithms http://blog.memsql.com/common-pitfalls-in-writing-lock-free-algorithms/

Toward generic atomic operations/The C11 memory model http://lwn.net/Articles/509102/

Ticket Spinlocks: http://lwn.net/Articles/267968/
Ticket implementation https://github.com/karthick18/ticket_spinlock/blob/master/spinlock.h



Lightweight Contention Management for
Efficient Compare-and-Swap Operations http://arxiv.org/pdf/1305.5800.pdf

MCSLocks http://lwn.net/Articles/590243/

Improving ticket spinlocks  http://lwn.net/Articles/531254/

http://ftp.cs.rochester.edu/u/scott/papers/2001_PPoPP_Timeout.pdf
